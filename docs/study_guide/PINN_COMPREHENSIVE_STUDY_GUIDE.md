# Physics-Informed Neural Networks (PINNs) - Comprehensive Study Guide
## From Theory to Implementation in Reservoir Modeling

---

## ğŸ“š **TABLE OF CONTENTS**

1. [Theoretical Foundations](#theoretical-foundations)
2. [Mathematical Framework](#mathematical-framework)
3. [Implementation Architecture](#implementation-architecture)
4. [Data Processing Pipeline](#data-processing-pipeline)
5. [Training Methodology](#training-methodology)
6. [Validation and Testing](#validation-and-testing)
7. [Real-World Applications](#real-world-applications)
8. [Advanced Topics](#advanced-topics)
9. [Troubleshooting Guide](#troubleshooting-guide)
10. [Research Directions](#research-directions)

---

## ğŸ¯ **LEARNING OBJECTIVES**

By the end of this study guide, you will be able to:

### **Theoretical Understanding**
- Explain the fundamental principles of physics-informed machine learning
- Derive the mathematical formulation of PINNs for reservoir modeling
- Understand automatic differentiation and its role in physics constraints
- Analyze convergence properties and approximation errors

### **Practical Implementation**
- Build a complete PINN system from scratch using PyTorch
- Process real LAS well log data for training
- Implement physics constraints for reservoir flow equations
- Design and execute training pipelines with stability guarantees

### **Real-World Application**
- Apply PINNs to actual petroleum engineering problems
- Validate models against industry standards and benchmarks
- Integrate PINN predictions into reservoir management workflows
- Assess model reliability and uncertainty for critical decisions

---

## ğŸ“– **CHAPTER 1: THEORETICAL FOUNDATIONS**

### **1.1 What are Physics-Informed Neural Networks?**

Physics-Informed Neural Networks (PINNs) represent a paradigm shift in scientific machine learning, combining the flexibility of neural networks with the rigor of physical laws.

#### **Core Concept**
```
Traditional ML: Data â†’ Model â†’ Predictions
Physics-Informed ML: Data + Physics Laws â†’ Model â†’ Physics-Consistent Predictions
```

#### **Key Advantages**
1. **Physics Compliance**: Predictions automatically satisfy governing equations
2. **Data Efficiency**: Learn from sparse, noisy measurements
3. **Extrapolation**: Reliable predictions outside training data range
4. **Interpretability**: Physics-based understanding of model behavior

#### **Mathematical Foundation**
A PINN seeks to approximate the solution u(x,t) to a partial differential equation:

```
F[u; Î»] = 0,  x âˆˆ Î©, t âˆˆ [0,T]
```

Subject to:
- **Initial conditions**: u(x,0) = uâ‚€(x)
- **Boundary conditions**: B[u] = 0 on âˆ‚Î©
- **Data constraints**: u(xáµ¢,táµ¢) = uáµ¢ for observed data points

### **1.2 Reservoir Modeling Context**

#### **Governing Equations for Reservoir Flow**

**Darcy's Law** (momentum conservation):
```
v = -(k/Î¼) âˆ‡p
```

**Continuity Equation** (mass conservation):
```
âˆ‡ Â· (Ïv) + âˆ‚(ÏÏ†)/âˆ‚t = q
```

**Saturation Constraint**:
```
Sâ‚’ + Sáµ¨ = 1
```

Where:
- v: Darcy velocity
- k: permeability
- Î¼: fluid viscosity
- p: pressure
- Ï: fluid density
- Ï†: porosity
- S: saturation
- q: source/sink terms

#### **PINN Formulation for Reservoir Modeling**

The neural network u_Î¸(x,y,z,t) approximates the solution vector:
```
u = [p(x,y,z,t), Sâ‚’(x,y,z,t), Sáµ¨(x,y,z,t)]
```

**Physics-Informed Loss Function**:
```
L = Î»_data Â· L_data + Î»_physics Â· L_physics + Î»_boundary Â· L_boundary

Where:
L_data = Î£áµ¢ ||u_Î¸(xáµ¢) - uáµ¢||Â²
L_physics = Î£â±¼ ||F[u_Î¸](xâ±¼)||Â²
L_boundary = Î£â‚– ||B[u_Î¸](xâ‚–)||Â²
```

### **1.3 Automatic Differentiation**

#### **Forward Mode AD**
Computes derivatives alongside function evaluation:
```python
# Example: f(x) = xÂ² + sin(x)
def forward_ad(x, dx=1.0):
    # Dual number arithmetic
    u = x * x + math.sin(x)
    du = 2*x*dx + math.cos(x)*dx
    return u, du
```

#### **Reverse Mode AD (Backpropagation)**
More efficient for functions with many inputs:
```python
# PyTorch automatic differentiation
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
u = torch.sum(x**2)
u.backward()
print(x.grad)  # [2.0, 4.0, 6.0]
```

#### **Higher-Order Derivatives**
Essential for PDE constraints:
```python
def compute_pde_residual(model, x, y):
    # First-order derivatives
    u = model(torch.cat([x, y], dim=1))
    u_x = torch.autograd.grad(u, x, create_graph=True)[0]
    u_y = torch.autograd.grad(u, y, create_graph=True)[0]
    
    # Second-order derivatives
    u_xx = torch.autograd.grad(u_x, x, create_graph=True)[0]
    u_yy = torch.autograd.grad(u_y, y, create_graph=True)[0]
    
    # Laplacian (for Darcy's law)
    laplacian = u_xx + u_yy
    return laplacian
```

---

## ğŸ”¢ **CHAPTER 2: MATHEMATICAL FRAMEWORK**

### **2.1 Universal Approximation Theory**

#### **Theoretical Foundation**
Neural networks can approximate any continuous function to arbitrary accuracy:

**Theorem (Hornik et al., 1989)**: Let Ïƒ be a non-constant, bounded, and continuous activation function. Then finite sums of the form:
```
G(x) = Î£áµ¢â‚Œâ‚á´º Î±áµ¢ Ïƒ(wáµ¢áµ€x + báµ¢)
```
are dense in C(K) for any compact set K âŠ‚ â„áµˆ.

#### **Implications for PINNs**
- Neural networks can approximate PDE solutions
- Approximation quality depends on network width and depth
- Physics constraints guide the approximation process

### **2.2 Convergence Analysis**

#### **Error Decomposition**
Total approximation error can be decomposed as:
```
||u - u_Î¸*||â‚‚ â‰¤ ||u - u*||â‚‚ + ||u* - u_Î¸*||â‚‚ + ||u_Î¸* - u_Î¸á´º||â‚‚

Where:
- u: true solution
- u*: best possible neural network approximation
- u_Î¸*: optimal parameters for given architecture
- u_Î¸á´º: parameters after N training iterations
```

#### **Convergence Rates**
For well-posed PDEs with Lipschitz continuous operators:

**Approximation Error**: O(1/âˆšwidth) for ReLU networks
**Optimization Error**: O(1/âˆšN) for SGD with appropriate learning rates
**Generalization Error**: O(âˆš(log(width)/n_data)) with high probability

### **2.3 Spectral Bias and Mitigation**

#### **Problem Statement**
Standard neural networks exhibit spectral bias toward low frequencies:
```python
# Demonstration of spectral bias
def high_freq_function(x):
    return torch.sin(20 * math.pi * x)

# Standard network struggles with high frequencies
# Solution: Fourier feature mapping
```

#### **Fourier Feature Networks**
Transform inputs to capture high-frequency components:
```python
class FourierFeatureNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, fourier_dim=256):
        super().__init__()
        self.fourier_dim = fourier_dim
        self.B = torch.randn(fourier_dim, input_dim) * 10.0
        
        self.network = nn.Sequential(
            nn.Linear(2 * fourier_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        # Fourier feature mapping
        x_proj = 2 * math.pi * x @ self.B.T
        fourier_features = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
        return self.network(fourier_features)
```

---

## ğŸ—ï¸ **CHAPTER 3: IMPLEMENTATION ARCHITECTURE**

### **3.1 System Overview**

#### **High-Level Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Layer    â”‚â”€â”€â”€â–¶â”‚  Model Layer    â”‚â”€â”€â”€â–¶â”‚ Training Layer  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ LAS Reader    â”‚    â”‚ â€¢ PINN Model    â”‚    â”‚ â€¢ Optimizer     â”‚
â”‚ â€¢ Preprocessor  â”‚    â”‚ â€¢ Physics Loss  â”‚    â”‚ â€¢ Scheduler     â”‚
â”‚ â€¢ Dataset       â”‚    â”‚ â€¢ Tensor Mgmt   â”‚    â”‚ â€¢ Monitor       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validation Layerâ”‚    â”‚ Visualization   â”‚    â”‚ Deployment      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Cross-Val     â”‚    â”‚ â€¢ Plotting      â”‚    â”‚ â€¢ API Server    â”‚
â”‚ â€¢ Physics Check â”‚    â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Model Store   â”‚
â”‚ â€¢ Benchmarking  â”‚    â”‚ â€¢ Analysis      â”‚    â”‚ â€¢ Inference     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3.2 Core Components**

#### **3.2.1 Data Processing Pipeline**

**LAS File Reader**:
```python
class LASReader:
    """Robust LAS file parser for well log data."""
    
    def __init__(self):
        self.supported_versions = ['2.0', '3.0']
        self.curve_mappings = {
            'GR': ['GR', 'GAMMA_RAY', 'GAPI'],
            'RHOB': ['RHOB', 'BULK_DENSITY', 'RHOZ'],
            'RT': ['RT', 'RESISTIVITY', 'ILD', 'RES']
        }
    
    def read_las_file(self, filepath):
        """Read and parse LAS file with error handling."""
        try:
            las = lasio.read(filepath)
            return self._extract_curves(las)
        except Exception as e:
            logger.error(f"Failed to read {filepath}: {e}")
            return None
    
    def _extract_curves(self, las):
        """Extract and standardize curve data."""
        curves = {}
        for standard_name, variants in self.curve_mappings.items():
            for variant in variants:
                if variant in las.curves:
                    curves[standard_name] = las[variant]
                    break
        return curves
```

**Data Preprocessor**:
```python
class DataPreprocessor:
    """Advanced preprocessing for well log data."""
    
    def __init__(self):
        self.outlier_method = 'percentile'
        self.interpolation_method = 'linear'
        self.normalization_method = 'robust'
    
    def preprocess(self, raw_data):
        """Complete preprocessing pipeline."""
        # 1. Quality control
        cleaned_data = self.remove_outliers(raw_data)
        
        # 2. Handle missing values
        interpolated_data = self.interpolate_missing(cleaned_data)
        
        # 3. Derive additional properties
        enhanced_data = self.derive_properties(interpolated_data)
        
        # 4. Normalize for training
        normalized_data = self.normalize(enhanced_data)
        
        return normalized_data
    
    def remove_outliers(self, data, percentile_range=(1, 99)):
        """Remove outliers using percentile-based method."""
        for column in data.columns:
            if data[column].dtype in ['float64', 'int64']:
                lower, upper = np.percentile(data[column].dropna(), percentile_range)
                data[column] = data[column].clip(lower, upper)
        return data
    
    def derive_properties(self, data):
        """Derive porosity and permeability from logs."""
        # Porosity from neutron-density
        if 'NPHI' in data.columns and 'RHOB' in data.columns:
            data['PORO'] = self.calculate_porosity(data['NPHI'], data['RHOB'])
        
        # Permeability from Kozeny-Carman
        if 'PORO' in data.columns:
            data['PERM'] = self.calculate_permeability(data['PORO'])
        
        return data
```

#### **3.2.2 PINN Model Architecture**

**Core PINN Implementation**:
```python
class PINNModel(nn.Module):
    """Physics-Informed Neural Network for reservoir modeling."""
    
    def __init__(self, input_dim=4, hidden_dims=[64, 64, 64], output_dim=2):
        super().__init__()
        
        # Input normalization
        self.input_norm = nn.BatchNorm1d(input_dim)
        
        # Hidden layers with residual connections
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.Tanh(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        
        self.hidden_layers = nn.Sequential(*layers)
        
        # Output layer with physics-appropriate activations
        self.pressure_head = nn.Sequential(
            nn.Linear(prev_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Normalized pressure
        )
        
        self.saturation_head = nn.Sequential(
            nn.Linear(prev_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Saturation âˆˆ [0,1]
        )
    
    def forward(self, x):
        """Forward pass through the network."""
        # Input: [depth, GR, porosity, permeability]
        x_norm = self.input_norm(x)
        features = self.hidden_layers(x_norm)
        
        pressure = self.pressure_head(features)
        saturation = self.saturation_head(features)
        
        return torch.cat([pressure, saturation], dim=1)
    
    def compute_physics_loss(self, x, predictions):
        """Compute physics-informed loss terms."""
        # Extract coordinates and properties
        depth = x[:, 0:1]
        porosity = x[:, 2:3]
        permeability = x[:, 3:4]
        
        pressure = predictions[:, 0:1]
        saturation = predictions[:, 1:2]
        
        # Compute gradients
        p_depth = torch.autograd.grad(
            pressure, depth, 
            grad_outputs=torch.ones_like(pressure),
            create_graph=True
        )[0]
        
        # Darcy's law: pressure should increase with depth
        darcy_residual = torch.abs(p_depth - 0.433)  # 0.433 psi/ft gradient
        
        # Porosity-pressure relationship
        porosity_constraint = torch.abs(
            pressure - (1.0 - porosity) * 0.5
        )
        
        # Saturation bounds
        saturation_constraint = torch.relu(-saturation) + torch.relu(saturation - 1.0)
        
        physics_loss = (
            darcy_residual.mean() + 
            porosity_constraint.mean() + 
            saturation_constraint.mean()
        )
        
        return physics_loss
```

#### **3.2.3 Training Pipeline**

**Advanced Training Strategy**:
```python
class PINNTrainer:
    """Comprehensive PINN training system."""
    
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        
        # Optimizers
        self.optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # Learning rate scheduler
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=50, factor=0.5
        )
        
        # Loss tracking
        self.train_losses = []
        self.val_losses = []
        self.physics_losses = []
        
        # Early stopping
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
    def train_epoch(self, epoch):
        """Train for one epoch with physics constraints."""
        self.model.train()
        total_loss = 0.0
        data_loss_total = 0.0
        physics_loss_total = 0.0
        
        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            inputs.requires_grad_(True)  # Enable gradient computation
            
            # Forward pass
            predictions = self.model(inputs)
            
            # Data loss
            data_loss = F.mse_loss(predictions, targets)
            
            # Physics loss
            physics_loss = self.model.compute_physics_loss(inputs, predictions)
            
            # Adaptive physics weighting
            physics_weight = self.get_physics_weight(epoch)
            
            # Combined loss
            total_batch_loss = data_loss + physics_weight * physics_loss
            
            # Backward pass
            self.optimizer.zero_grad()
            total_batch_loss.backward()
            
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            
            self.optimizer.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            data_loss_total += data_loss.item()
            physics_loss_total += physics_loss.item()
        
        # Average losses
        avg_total_loss = total_loss / len(self.train_loader)
        avg_data_loss = data_loss_total / len(self.train_loader)
        avg_physics_loss = physics_loss_total / len(self.train_loader)
        
        return avg_total_loss, avg_data_loss, avg_physics_loss
    
    def get_physics_weight(self, epoch):
        """Adaptive physics weight scheduling."""
        if epoch < 200:
            return 0.001  # Start with small physics weight
        elif epoch < 600:
            # Gradually increase physics importance
            progress = (epoch - 200) / 400
            return 0.001 + progress * (0.01 - 0.001)
        else:
            return 0.01  # Full physics weight
    
    def validate(self):
        """Validation with comprehensive metrics."""
        self.model.eval()
        val_loss = 0.0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for inputs, targets in self.val_loader:
                predictions = self.model(inputs)
                loss = F.mse_loss(predictions, targets)
                val_loss += loss.item()
                
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(targets.cpu().numpy())
        
        # Compute detailed metrics
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        pressure_r2 = r2_score(targets[:, 0], predictions[:, 0])
        saturation_r2 = r2_score(targets[:, 1], predictions[:, 1])
        
        avg_val_loss = val_loss / len(self.val_loader)
        
        return avg_val_loss, pressure_r2, saturation_r2
    
    def train(self, num_epochs):
        """Complete training loop with monitoring."""
        for epoch in range(num_epochs):
            # Training
            train_loss, data_loss, physics_loss = self.train_epoch(epoch)
            
            # Validation
            val_loss, pressure_r2, saturation_r2 = self.validate()
            
            # Learning rate scheduling
            self.scheduler.step(val_loss)
            
            # Early stopping check
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                self.patience_counter += 1
            
            # Logging
            if epoch % 50 == 0:
                print(f"Epoch {epoch:4d} | "
                      f"Train: {train_loss:.6f} | "
                      f"Val: {val_loss:.6f} | "
                      f"P_RÂ²: {pressure_r2:.3f} | "
                      f"S_RÂ²: {saturation_r2:.3f}")
            
            # Early stopping
            if self.patience_counter >= self.config.patience:
                print(f"Early stopping at epoch {epoch}")
                break
        
        # Load best model
        self.model.load_state_dict(torch.load('best_model.pth'))
        return self.model
```

---

## ğŸ“Š **CHAPTER 4: DATA PROCESSING PIPELINE**

### **4.1 LAS File Processing**

#### **Understanding LAS Format**
Log ASCII Standard (LAS) files contain well log data in a structured format:

```
~Version Information
VERS.   2.0 :   CWLS LOG ASCII STANDARD -VERSION 2.0
WRAP.   NO  :   ONE LINE PER DEPTH STEP

~Well Information
STRT.M  1670.000000 :START DEPTH
STOP.M  1669.750000 :STOP DEPTH
STEP.M  -0.125000   :STEP
NULL.   -999.25     :NULL VALUE

~Curve Information
DEPT.M      : 1  DEPTH
GR.GAPI     : 2  GAMMA RAY
RHOB.G/C3   : 3  BULK DENSITY
RT.OHMM     : 4  RESISTIVITY

~ASCII
1670.000  45.234  2.350  12.456
1669.875  46.123  2.340  13.234
...
```

#### **Robust Parsing Implementation**
```python
class AdvancedLASReader:
    """Production-grade LAS file reader with comprehensive error handling."""
    
    def __init__(self):
        self.null_values = [-999.25, -9999, -999, 9999, None, np.nan]
        self.curve_aliases = {
            'GR': ['GR', 'GAMMA_RAY', 'GAPI', 'SGR', 'GAMMA'],
            'RHOB': ['RHOB', 'BULK_DENSITY', 'RHOZ', 'DEN', 'DENSITY'],
            'RT': ['RT', 'RESISTIVITY', 'ILD', 'RES', 'RESIST'],
            'NPHI': ['NPHI', 'NEUTRON', 'NEU', 'NEUTRON_POROSITY']
        }
    
    def read_with_validation(self, filepath):
        """Read LAS file with comprehensive validation."""
        try:
            # Basic file reading
            las = lasio.read(filepath)
            
            # Validation checks
            validation_results = self.validate_las_file(las)
            if not validation_results['is_valid']:
                logger.warning(f"Validation issues in {filepath}: {validation_results['issues']}")
            
            # Extract and clean data
            data = self.extract_clean_data(las)
            
            # Quality assessment
            quality_score = self.assess_data_quality(data)
            
            return {
                'data': data,
                'quality_score': quality_score,
                'validation': validation_results,
                'metadata': self.extract_metadata(las)
            }
            
        except Exception as e:
            logger.error(f"Failed to process {filepath}: {str(e)}")
            return None
    
    def validate_las_file(self, las):
        """Comprehensive LAS file validation."""
        issues = []
        
        # Check depth coverage
        depth_range = las.index[-1] - las.index[0]
        if abs(depth_range) < 100:  # Less than 100 ft
            issues.append("Insufficient depth coverage")
        
        # Check for required curves
        available_curves = [curve.mnemonic for curve in las.curves]
        required_curves = ['GR', 'RHOB', 'RT']
        
        for req_curve in required_curves:
            if not any(alias in available_curves for alias in self.curve_aliases[req_curve]):
                issues.append(f"Missing required curve: {req_curve}")
        
        # Check data completeness
        for curve in las.curves:
            if curve.mnemonic != 'DEPT':
                null_percentage = self.calculate_null_percentage(las[curve.mnemonic])
                if null_percentage > 50:
                    issues.append(f"High null percentage in {curve.mnemonic}: {null_percentage:.1f}%")
        
        return {
            'is_valid': len(issues) == 0,
            'issues': issues,
            'depth_range': depth_range,
            'curve_count': len(available_curves)
        }
    
    def extract_clean_data(self, las):
        """Extract and clean curve data."""
        data = {}
        
        # Extract depth
        data['DEPTH'] = las.index
        
        # Extract curves with alias mapping
        for standard_name, aliases in self.curve_aliases.items():
            for alias in aliases:
                if alias in [curve.mnemonic for curve in las.curves]:
                    raw_curve = las[alias]
                    cleaned_curve = self.clean_curve_data(raw_curve)
                    data[standard_name] = cleaned_curve
                    break
        
        # Convert to DataFrame
        df = pd.DataFrame(data)
        
        # Remove rows with excessive nulls
        df = df.dropna(thresh=len(df.columns) * 0.7)  # Keep rows with <30% nulls
        
        return df
    
    def clean_curve_data(self, curve_data):
        """Clean individual curve data."""
        # Replace null values
        cleaned = np.array(curve_data)
        for null_val in self.null_values:
            if null_val is not None and not np.isnan(null_val):
                cleaned[cleaned == null_val] = np.nan
        
        # Remove extreme outliers (beyond 5 standard deviations)
        if len(cleaned[~np.isnan(cleaned)]) > 10:
            mean_val = np.nanmean(cleaned)
            std_val = np.nanstd(cleaned)
            outlier_mask = np.abs(cleaned - mean_val) > 5 * std_val
            cleaned[outlier_mask] = np.nan
        
        return cleaned
```

### **4.2 Feature Engineering**

#### **Derived Property Calculations**
```python
class ReservoirPropertyCalculator:
    """Calculate reservoir properties from well logs."""
    
    def __init__(self):
        self.porosity_models = {
            'neutron_density': self.neutron_density_porosity,
            'density_only': self.density_porosity,
            'neutron_only': self.neutron_porosity
        }
        
        self.permeability_models = {
            'kozeny_carman': self.kozeny_carman_permeability,
            'timur': self.timur_permeability,
            'coates': self.coates_permeability
        }
    
    def neutron_density_porosity(self, nphi, rhob, matrix_density=2.65, fluid_density=1.0):
        """Calculate porosity from neutron and density logs."""
        # Density porosity
        phi_d = (matrix_density - rhob) / (matrix_density - fluid_density)
        
        # Neutron porosity (already in porosity units)
        phi_n = nphi
        
        # Combined porosity (geometric mean for gas correction)
        phi_combined = np.sqrt(phi_d * phi_n)
        
        # Apply realistic bounds
        phi_combined = np.clip(phi_combined, 0.01, 0.35)
        
        return phi_combined
    
    def kozeny_carman_permeability(self, porosity, grain_size=0.1):
        """Calculate permeability using Kozeny-Carman equation."""
        # K = (phi^3 / (1-phi)^2) * (d^2 / 180)
        # where d is grain diameter in mm, K in mD
        
        phi = np.clip(porosity, 0.01, 0.35)  # Avoid division by zero
        
        permeability = (phi**3 / (1 - phi)**2) * (grain_size**2 / 180) * 1000  # Convert to mD
        
        # Apply realistic bounds for reservoir rocks
        permeability = np.clip(permeability, 0.01, 10000)
        
        return permeability
    
    def calculate_water_saturation(self, rt, porosity, formation_factor=1.0, water_resistivity=0.1):
        """Calculate water saturation using Archie's equation."""
        # Sw = ((a * Rw) / (phi^m * Rt))^(1/n)
        # Simplified: assume a=1, m=2, n=2
        
        phi = np.clip(porosity, 0.01, 0.35)
        rt_clean = np.clip(rt, 0.1, 1000)
        
        sw = np.sqrt((formation_factor * water_resistivity) / (phi**2 * rt_clean))
        sw = np.clip(sw, 0.0, 1.0)
        
        return sw
    
    def generate_realistic_targets(self, depth, porosity, permeability):
        """Generate realistic pressure and saturation targets."""
        # Pressure: hydrostatic + formation pressure
        hydrostatic_gradient = 0.433  # psi/ft for water
        formation_gradient = 0.45     # slightly overpressured
        
        pressure = depth * formation_gradient / 1000  # Normalize to [0,1]
        
        # Add porosity effect (higher porosity = slightly lower pressure)
        pressure_adjustment = -0.1 * (porosity - 0.15)
        pressure += pressure_adjustment
        
        # Saturation: function of depth and permeability
        base_saturation = 0.7  # 70% water saturation
        depth_effect = -0.0001 * depth  # Deeper = less water
        perm_effect = 0.05 * np.log10(np.clip(permeability, 0.1, 1000)) / 3  # Higher perm = more oil
        
        saturation = base_saturation + depth_effect + perm_effect
        saturation = np.clip(saturation, 0.2, 0.9)
        
        return pressure, saturation
```

### **4.3 Data Quality Assessment**

#### **Comprehensive Quality Metrics**
```python
class DataQualityAssessor:
    """Assess and score data quality for PINN training."""
    
    def __init__(self):
        self.quality_thresholds = {
            'completeness': 0.8,      # 80% non-null values
            'consistency': 0.9,       # 90% physically reasonable
            'continuity': 0.85,       # 85% continuous measurements
            'accuracy': 0.75          # 75% within expected ranges
        }
    
    def assess_well_quality(self, well_data):
        """Comprehensive quality assessment for a single well."""
        scores = {}
        
        # 1. Completeness Score
        scores['completeness'] = self.calculate_completeness(well_data)
        
        # 2. Physical Consistency Score
        scores['consistency'] = self.calculate_consistency(well_data)
        
        # 3. Data Continuity Score
        scores['continuity'] = self.calculate_continuity(well_data)
        
        # 4. Range Accuracy Score
        scores['accuracy'] = self.calculate_accuracy(well_data)
        
        # 5. Overall Quality Score (weighted average)
        weights = {'completeness': 0.3, 'consistency': 0.3, 'continuity': 0.2, 'accuracy': 0.2}
        overall_score = sum(scores[metric] * weights[metric] for metric in scores)
        
        return {
            'individual_scores': scores,
            'overall_score': overall_score,
            'quality_grade': self.grade_quality(overall_score),
            'recommendations': self.generate_recommendations(scores)
        }
    
    def calculate_completeness(self, data):
        """Calculate data completeness score."""
        total_points = len(data)
        if total_points == 0:
            return 0.0
        
        completeness_scores = []
        for column in ['GR', 'RHOB', 'RT', 'NPHI']:
            if column in data.columns:
                non_null_count = data[column].notna().sum()
                completeness_scores.append(non_null_count / total_points)
        
        return np.mean(completeness_scores) if completeness_scores else 0.0
    
    def calculate_consistency(self, data):
        """Calculate physical consistency score."""
        consistency_checks = []
        
        # Gamma Ray: typically 0-200 GAPI
        if 'GR' in data.columns:
            gr_valid = ((data['GR'] >= 0) & (data['GR'] <= 300)).sum()
            consistency_checks.append(gr_valid / len(data))
        
        # Bulk Density: typically 1.5-3.0 g/cc
        if 'RHOB' in data.columns:
            rhob_valid = ((data['RHOB'] >= 1.0) & (data['RHOB'] <= 3.5)).sum()
            consistency_checks.append(rhob_valid / len(data))
        
        # Resistivity: typically 0.1-1000 ohm-m
        if 'RT' in data.columns:
            rt_valid = ((data['RT'] >= 0.01) & (data['RT'] <= 10000)).sum()
            consistency_checks.append(rt_valid / len(data))
        
        return np.mean(consistency_checks) if consistency_checks else 0.0
    
    def calculate_continuity(self, data):
        """Calculate data continuity score."""
        if 'DEPTH' not in data.columns or len(data) < 10:
            return 0.0
        
        # Check depth spacing consistency
        depth_diffs = np.diff(data['DEPTH'].values)
        median_spacing = np.median(np.abs(depth_diffs))
        
        # Count points with reasonable spacing (within 2x median)
        reasonable_spacing = np.abs(depth_diffs) <= 2 * median_spacing
        continuity_score = reasonable_spacing.sum() / len(depth_diffs)
        
        return continuity_score
    
    def grade_quality(self, score):
        """Assign quality grade based on score."""
        if score >= 0.9:
            return 'A'  # Excellent
        elif score >= 0.8:
            return 'B'  # Good
        elif score >= 0.7:
            return 'C'  # Fair
        elif score >= 0.6:
            return 'D'  # Poor
        else:
            return 'F'  # Fail
```

---

## ğŸ¯ **CHAPTER 5: TRAINING METHODOLOGY**

### **5.1 Multi-Phase Training Strategy**

#### **Phase 1: Data Fitting (Epochs 1-200)**
Focus on learning data patterns without physics constraints:

```python
def phase1_training(model, data_loader, epochs=200):
    """Phase 1: Pure data fitting to establish baseline."""
    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)
    
    for epoch in range(epochs):
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            optimizer.zero_grad()
            
            predictions = model(inputs)
            
            # Only data loss in Phase 1
            loss = F.mse_loss(predictions, targets)
            
            loss.backward()
            optimizer.step()
        
        if epoch % 50 == 0:
            print(f"Phase 1 - Epoch {epoch}: Loss = {loss.item():.6f}")
```

#### **Phase 2: Physics Integration (Epochs 201-800)**
Gradually introduce physics constraints:

```python
def phase2_training(model, data_loader, epochs=600, start_epoch=200):
    """Phase 2: Gradual physics integration."""
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
    
    for epoch in range(start_epoch, start_epoch + epochs):
        # Adaptive physics weight
        progress = (epoch - start_epoch) / epochs
        physics_weight = 0.001 + progress * (0.01 - 0.001)
        
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            inputs.requires_grad_(True)
            optimizer.zero_grad()
            
            predictions = model(inputs)
            
            # Data loss
            data_loss = F.mse_loss(predictions, targets)
            
            # Physics loss
            physics_loss = model.compute_physics_loss(inputs, predictions)
            
            # Combined loss
            total_loss = data_loss + physics_weight * physics_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
```

#### **Phase 3: Fine-tuning (Epochs 801+)**
Balanced optimization with full physics constraints:

```python
def phase3_training(model, data_loader, epochs=400, start_epoch=800):
    """Phase 3: Balanced fine-tuning."""
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50)
    
    physics_weight = 0.01  # Full physics weight
    
    for epoch in range(start_epoch, start_epoch + epochs):
        epoch_loss = 0.0
        
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            inputs.requires_grad_(True)
            optimizer.zero_grad()
            
            predictions = model(inputs)
            
            data_loss = F.mse_loss(predictions, targets)
            physics_loss = model.compute_physics_loss(inputs, predictions)
            
            total_loss = data_loss + physics_weight * physics_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()
            
            epoch_loss += total_loss.item()
        
        # Learning rate scheduling
        avg_loss = epoch_loss / len(data_loader)
        scheduler.step(avg_loss)
```

### **5.2 Advanced Optimization Techniques**

#### **L-BFGS Refinement**
For final precision improvement:

```python
def lbfgs_refinement(model, data_loader, max_iter=100):
    """L-BFGS refinement for final precision."""
    
    # Convert data to single batch for L-BFGS
    all_inputs = []
    all_targets = []
    for inputs, targets in data_loader:
        all_inputs.append(inputs)
        all_targets.append(targets)
    
    full_inputs = torch.cat(all_inputs, dim=0)
    full_targets = torch.cat(all_targets, dim=0)
    full_inputs.requires_grad_(True)
    
    optimizer = torch.optim.LBFGS(
        model.parameters(),
        max_iter=max_iter,
        tolerance_grad=1e-7,
        tolerance_change=1e-9,
        history_size=100
    )
    
    def closure():
        optimizer.zero_grad()
        predictions = model(full_inputs)
        
        data_loss = F.mse_loss(predictions, full_targets)
        physics_loss = model.compute_physics_loss(full_inputs, predictions)
        
        total_loss = data_loss + 0.01 * physics_loss
        total_loss.backward()
        
        return total_loss
    
    optimizer.step(closure)
    
    return model
```

### **5.3 Numerical Stability Techniques**

#### **Gradient Scaling and Clipping**
```python
class StabilizedTraining:
    """Advanced numerical stability for PINN training."""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.scaler = torch.cuda.amp.GradScaler()  # Mixed precision
        
    def stable_backward(self, loss, inputs):
        """Numerically stable backward pass."""
        # Check for NaN in loss
        if torch.isnan(loss):
            logger.warning("NaN loss detected, skipping backward pass")
            return False
        
        # Scale loss for mixed precision
        scaled_loss = self.scaler.scale(loss)
        scaled_loss.backward()
        
        # Check gradients before clipping
        total_norm = 0.0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1. / 2)
        
        if total_norm > 100:  # Gradient explosion detection
            logger.warning(f"Large gradient norm detected: {total_norm:.2f}")
        
        # Unscale gradients and clip
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
        
        return True
    
    def adaptive_physics_weight(self, epoch, data_loss, physics_loss):
        """Adaptive physics weight based on loss magnitudes."""
        if physics_loss.item() == 0:
            return 0.001
        
        # Balance loss magnitudes
        ratio = data_loss.item() / physics_loss.item()
        
        # Base weight from schedule
        base_weight = self.get_scheduled_weight(epoch)
        
        # Adaptive adjustment
        if ratio > 100:  # Physics loss too small
            adaptive_weight = base_weight * 2
        elif ratio < 0.1:  # Physics loss too large
            adaptive_weight = base_weight * 0.5
        else:
            adaptive_weight = base_weight
        
        return np.clip(adaptive_weight, 0.0001, 0.1)
```

---

This comprehensive study guide provides the theoretical foundation, mathematical framework, and practical implementation details needed to understand and implement PINNs for reservoir modeling. The content is structured for progressive learning, from basic concepts to advanced techniques.

Would you like me to continue with the remaining chapters (Validation, Applications, Advanced Topics, etc.) and create the detailed architecture flowcharts?