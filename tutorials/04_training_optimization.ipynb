{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks for Reservoir Modeling\n",
    "## Tutorial 4: Training Optimization and Best Practices\n",
    "\n",
    "In this tutorial, we'll explore advanced training strategies for PINNs, including multi-phase optimization, adaptive loss weighting, and convergence monitoring. We'll implement the complete training pipeline used in production PINN applications.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Implement two-phase optimization (Adam + L-BFGS)\n",
    "- Use adaptive loss weighting strategies\n",
    "- Monitor convergence and implement early stopping\n",
    "- Handle training instabilities and numerical issues\n",
    "- Optimize hyperparameters for PINN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from training.pinn_trainer import PINNTrainer\n",
    "from training.optimizer_manager import OptimizerManager\n",
    "from training.batch_processor import BatchProcessor\n",
    "from training.convergence_monitor import ConvergenceMonitor\n",
    "from models.pinn_architecture import PINNArchitecture\n",
    "from physics.physics_loss import PhysicsLossCalculator\n",
    "from visualization.training_visualizer import TrainingVisualizer\n",
    "\n",
    "# Set up device and plotting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Results\n",
    "\n",
    "Let's start by loading the model and data from previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and previous model\n",
    "data_path = Path('../output/pinn_datasets.pkl')\n",
    "model_path = Path('../output/pinn_model_demo.pth')\n",
    "\n",
    "# Load data\n",
    "if data_path.exists():\n",
    "    with open(data_path, 'rb') as f:\n",
    "        pinn_data = pickle.load(f)\n",
    "    print(\"üìÇ Loaded datasets from previous tutorials\")\n",
    "else:\n",
    "    print(\"üîß Creating synthetic datasets...\")\n",
    "    # Create synthetic data (same as Tutorial 3)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    def create_synthetic_data(n_samples):\n",
    "        depth = np.random.uniform(2000, 2500, n_samples)\n",
    "        gr = 30 + 50 * np.sin(0.01 * depth) + 10 * np.random.randn(n_samples)\n",
    "        phie = 0.15 + 0.05 * np.random.randn(n_samples)\n",
    "        perm = 10 * np.exp(2 * (phie - 0.15)) * np.exp(0.5 * np.random.randn(n_samples))\n",
    "        \n",
    "        X = np.column_stack([depth, gr, phie, perm])\n",
    "        \n",
    "        depth_norm = (depth - 2000) / 500\n",
    "        pressure = 100 + 50 * depth_norm + 20 * (1 - phie) + 5 * np.random.randn(n_samples)\n",
    "        saturation = 0.3 + 0.4 * phie + 0.1 * np.random.randn(n_samples)\n",
    "        saturation = np.clip(saturation, 0.2, 0.8)\n",
    "        \n",
    "        y = np.column_stack([pressure, saturation])\n",
    "        well_ids = [f'WELL_{i//100:02d}' for i in range(n_samples)]\n",
    "        \n",
    "        return {\n",
    "            'X': X, 'y': y, 'well_ids': well_ids,\n",
    "            'feature_names': ['depth', 'GR', 'PHIE', 'PERM'],\n",
    "            'target_names': ['pressure', 'saturation'],\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "    \n",
    "    pinn_data = {\n",
    "        'train': create_synthetic_data(2000),\n",
    "        'validation': create_synthetic_data(500),\n",
    "        'test': create_synthetic_data(300)\n",
    "    }\n",
    "\n",
    "print(f\"üìä Dataset loaded: {pinn_data['train']['n_samples']} train, {pinn_data['validation']['n_samples']} val, {pinn_data['test']['n_samples']} test\")"
   ]
  }  {
 
  "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Training Configuration\n",
    "\n",
    "Let's set up a comprehensive training configuration that includes all the best practices for PINN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced training configuration\n",
    "training_config = {\n",
    "    # Model architecture\n",
    "    'model': {\n",
    "        'input_dim': 4,\n",
    "        'hidden_dims': [64, 64, 64, 64],\n",
    "        'output_dim': 2,\n",
    "        'activation': 'tanh',\n",
    "        'initialization': 'xavier_normal'\n",
    "    },\n",
    "    \n",
    "    # Training phases\n",
    "    'training': {\n",
    "        'phase1': {\n",
    "            'optimizer': 'adam',\n",
    "            'lr': 1e-3,\n",
    "            'epochs': 2000,\n",
    "            'batch_size': 256,\n",
    "            'scheduler': 'cosine_annealing'\n",
    "        },\n",
    "        'phase2': {\n",
    "            'optimizer': 'lbfgs',\n",
    "            'lr': 1.0,\n",
    "            'max_iter': 1000,\n",
    "            'tolerance_grad': 1e-7,\n",
    "            'tolerance_change': 1e-9\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Loss configuration\n",
    "    'loss': {\n",
    "        'weights': {\n",
    "            'data': 1.0,\n",
    "            'physics': 0.1,\n",
    "            'boundary': 0.5\n",
    "        },\n",
    "        'adaptive_weighting': True,\n",
    "        'weight_update_frequency': 100\n",
    "    },\n",
    "    \n",
    "    # Convergence monitoring\n",
    "    'convergence': {\n",
    "        'patience': 200,\n",
    "        'min_delta': 1e-6,\n",
    "        'monitor_metric': 'val_loss',\n",
    "        'restore_best_weights': True\n",
    "    },\n",
    "    \n",
    "    # Numerical stability\n",
    "    'stability': {\n",
    "        'gradient_clipping': 1.0,\n",
    "        'nan_detection': True,\n",
    "        'loss_scaling': False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Advanced Training Configuration:\")\n",
    "print(f\"  Phase 1: {training_config['training']['phase1']['optimizer'].upper()} for {training_config['training']['phase1']['epochs']} epochs\")\n",
    "print(f\"  Phase 2: {training_config['training']['phase2']['optimizer'].upper()} for {training_config['training']['phase2']['max_iter']} iterations\")\n",
    "print(f\"  Adaptive loss weighting: {training_config['loss']['adaptive_weighting']}\")\n",
    "print(f\"  Early stopping patience: {training_config['convergence']['patience']}\")"
   ]
  }  {
 
  "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Advanced Training Components\n",
    "\n",
    "Let's create all the components needed for advanced PINN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training components\n",
    "print(\"üèóÔ∏è Initializing Advanced Training Components\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create fresh model\n",
    "model = PINNArchitecture(\n",
    "    input_dim=training_config['model']['input_dim'],\n",
    "    hidden_dims=training_config['model']['hidden_dims'],\n",
    "    output_dim=training_config['model']['output_dim'],\n",
    "    activation=training_config['model']['activation']\n",
    ").to(device)\n",
    "\n",
    "# Initialize training components\n",
    "trainer = PINNTrainer(model, device)\n",
    "optimizer_manager = OptimizerManager()\n",
    "batch_processor = BatchProcessor(device)\n",
    "convergence_monitor = ConvergenceMonitor(\n",
    "    patience=training_config['convergence']['patience'],\n",
    "    min_delta=training_config['convergence']['min_delta']\n",
    ")\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "print(f\"‚úÖ Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"‚úÖ Training components initialized\")\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_training_data(pinn_data, device):\n",
    "    \"\"\"Prepare all training data tensors\"\"\"\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_X = torch.FloatTensor(pinn_data['train']['X']).to(device)\n",
    "    train_y = torch.FloatTensor(pinn_data['train']['y']).to(device)\n",
    "    val_X = torch.FloatTensor(pinn_data['validation']['X']).to(device)\n",
    "    val_y = torch.FloatTensor(pinn_data['validation']['y']).to(device)\n",
    "    \n",
    "    # Physics points (subset for efficiency)\n",
    "    n_physics = min(1000, len(train_X))\n",
    "    physics_idx = torch.randperm(len(train_X))[:n_physics]\n",
    "    physics_X = train_X[physics_idx].clone().requires_grad_(True)\n",
    "    \n",
    "    # Boundary conditions\n",
    "    n_boundary = 100\n",
    "    boundary_X = train_X[:n_boundary].clone()\n",
    "    boundary_y = torch.zeros(n_boundary, 2, device=device)\n",
    "    boundary_y[:, 0] = 100.0  # Fixed pressure\n",
    "    boundary_y[:, 1] = 0.5    # Fixed saturation\n",
    "    \n",
    "    return {\n",
    "        'train': (train_X, train_y),\n",
    "        'validation': (val_X, val_y),\n",
    "        'physics': physics_X,\n",
    "        'boundary': (boundary_X, boundary_y)\n",
    "    }\n",
    "\n",
    "training_data = prepare_training_data(pinn_data, device)\n",
    "print(f\"üì¶ Training data prepared: {len(training_data['train'][0])} samples\")"
   ]
  }  {
  
 "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive Loss Weighting Strategy\n",
    "\n",
    "One of the key challenges in PINN training is balancing different loss terms. Let's implement an adaptive weighting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLossWeighting:\n",
    "    \"\"\"Adaptive loss weighting for PINN training\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_weights, update_frequency=100, alpha=0.9):\n",
    "        self.weights = initial_weights.copy()\n",
    "        self.update_frequency = update_frequency\n",
    "        self.alpha = alpha  # Exponential moving average factor\n",
    "        self.loss_history = {key: [] for key in initial_weights.keys()}\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def update_weights(self, loss_components):\n",
    "        \"\"\"Update loss weights based on loss magnitudes\"\"\"\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Record loss components\n",
    "        for key, value in loss_components.items():\n",
    "            if key in self.loss_history:\n",
    "                self.loss_history[key].append(value)\n",
    "        \n",
    "        # Update weights periodically\n",
    "        if self.step_count % self.update_frequency == 0:\n",
    "            self._rebalance_weights()\n",
    "    \n",
    "    def _rebalance_weights(self):\n",
    "        \"\"\"Rebalance weights based on recent loss history\"\"\"\n",
    "        \n",
    "        if len(self.loss_history['data']) < 10:\n",
    "            return\n",
    "        \n",
    "        # Calculate recent average losses\n",
    "        recent_losses = {}\n",
    "        for key in self.weights.keys():\n",
    "            if key in self.loss_history and len(self.loss_history[key]) > 0:\n",
    "                recent_losses[key] = np.mean(self.loss_history[key][-10:])\n",
    "        \n",
    "        # Normalize by data loss (reference)\n",
    "        if 'data' in recent_losses and recent_losses['data'] > 0:\n",
    "            reference_loss = recent_losses['data']\n",
    "            \n",
    "            for key in self.weights.keys():\n",
    "                if key != 'data' and key in recent_losses:\n",
    "                    # Adjust weight inversely proportional to loss magnitude\n",
    "                    ratio = recent_losses[key] / reference_loss\n",
    "                    if ratio > 0:\n",
    "                        new_weight = self.weights[key] / max(ratio, 0.1)\n",
    "                        # Smooth update\n",
    "                        self.weights[key] = self.alpha * self.weights[key] + (1 - self.alpha) * new_weight\n",
    "        \n",
    "        print(f\"  üìä Updated weights: {', '.join([f'{k}={v:.3f}' for k, v in self.weights.items()])}\")\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights.copy()\n",
    "\n",
    "# Initialize adaptive weighting\n",
    "adaptive_weighting = AdaptiveLossWeighting(\n",
    "    initial_weights=training_config['loss']['weights'],\n",
    "    update_frequency=training_config['loss']['weight_update_frequency']\n",
    ")\n",
    "\n",
    "print(\"üéØ Adaptive Loss Weighting initialized\")\n",
    "print(f\"   Initial weights: {adaptive_weighting.get_weights()}\")\n",
    "print(f\"   Update frequency: {training_config['loss']['weight_update_frequency']} steps\")"
   ]
  }  {

   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Training Pipeline Implementation\n",
    "\n",
    "Now let's implement the complete two-phase training pipeline with all advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_pinn_training(model, training_data, config, adaptive_weighting, convergence_monitor):\n",
    "    \"\"\"Complete advanced PINN training pipeline\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Advanced PINN Training\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Unpack training data\n",
    "    train_X, train_y = training_data['train']\n",
    "    val_X, val_y = training_data['validation']\n",
    "    physics_X = training_data['physics']\n",
    "    boundary_X, boundary_y = training_data['boundary']\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], 'data_loss': [],\n",
    "        'physics_loss': [], 'boundary_loss': [], 'lr': [],\n",
    "        'weights': {'data': [], 'physics': [], 'boundary': []}\n",
    "    }\n",
    "    \n",
    "    # Phase 1: Adam Optimization\n",
    "    print(\"\\nüìà Phase 1: Adam Optimization\")\n",
    "    phase1_config = config['training']['phase1']\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=phase1_config['lr'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=phase1_config['epochs'], eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    batch_size = phase1_config['batch_size']\n",
    "    n_batches = len(train_X) // batch_size\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(phase1_config['epochs']):\n",
    "        model.train()\n",
    "        epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}\n",
    "        \n",
    "        # Batch training\n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(train_X))\n",
    "            \n",
    "            batch_X = train_X[start_idx:end_idx]\n",
    "            batch_y = train_y[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute losses\n",
    "            current_weights = adaptive_weighting.get_weights()\n",
    "            \n",
    "            # Data loss\n",
    "            pred_y = model(batch_X)\n",
    "            data_loss = torch.nn.functional.mse_loss(pred_y, batch_y)\n",
    "            \n",
    "            # Physics loss (simplified)\n",
    "            physics_pred = model(physics_X[:batch_size])\n",
    "            physics_loss = torch.mean(torch.abs(physics_pred))\n",
    "            \n",
    "            # Boundary loss\n",
    "            boundary_pred = model(boundary_X[:batch_size//4])\n",
    "            boundary_loss = torch.nn.functional.mse_loss(\n",
    "                boundary_pred, boundary_y[:batch_size//4]\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = (current_weights['data'] * data_loss + \n",
    "                         current_weights['physics'] * physics_loss + \n",
    "                         current_weights['boundary'] * boundary_loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if config['stability']['gradient_clipping']:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), config['stability']['gradient_clipping']\n",
    "                )\n",
    "            \n",
    "            # NaN detection\n",
    "            if config['stability']['nan_detection']:\n",
    "                if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                    print(f\"‚ö†Ô∏è  NaN/Inf detected at epoch {epoch}, batch {batch_idx}\")\n",
    "                    break\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            epoch_losses['total'] += total_loss.item()\n",
    "            epoch_losses['data'] += data_loss.item()\n",
    "            epoch_losses['physics'] += physics_loss.item()\n",
    "            epoch_losses['boundary'] += boundary_loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= n_batches\n",
    "        \n",
    "        # Update adaptive weights\n",
    "        if config['loss']['adaptive_weighting']:\n",
    "            adaptive_weighting.update_weights(epoch_losses)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_X)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_pred, val_y).item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(epoch_losses['total'])\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['data_loss'].append(epoch_losses['data'])\n",
    "        history['physics_loss'].append(epoch_losses['physics'])\n",
    "        history['boundary_loss'].append(epoch_losses['boundary'])\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        current_weights = adaptive_weighting.get_weights()\n",
    "        for key in history['weights']:\n",
    "            history['weights'][key].append(current_weights[key])\n",
    "        \n",
    "        # Early stopping check\n",
    "        if convergence_monitor.should_stop(val_loss):\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if convergence_monitor.early_stop:\n",
    "            print(f\"\\nüõë Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        # Progress reporting\n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:4d}: Loss={epoch_losses['total']:.6f}, \"\n",
    "                  f\"Val={val_loss:.6f}, LR={current_lr:.2e}\")\n",
    "    \n",
    "    # Restore best model if early stopping\n",
    "    if config['convergence']['restore_best_weights'] and best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"‚úÖ Restored best model (val_loss={best_val_loss:.6f})\")\n",
    "    \n",
    "    print(f\"\\nüìä Phase 1 Complete: {len(history['train_loss'])} epochs\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run advanced training\n",
    "training_history = advanced_pinn_training(\n",
    "    model, training_data, training_config, adaptive_weighting, convergence_monitor\n",
    ")"
   ]
  }