{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks for Reservoir Modeling\n",
    "## Tutorial 2: Data Processing and LAS File Handling\n",
    "\n",
    "In this tutorial, we'll learn how to process real Kansas Geological Survey (KGS) LAS well log files and prepare quality datasets for PINN training. This is a crucial step that determines the success of our physics-informed models.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Read and parse LAS files with different formats and versions\n",
    "- Extract and validate well log curves (gamma ray, density, neutron porosity, resistivity)\n",
    "- Implement robust data preprocessing and quality filtering\n",
    "- Create training/validation datasets for PINN models\n",
    "- Visualize data distributions and identify quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.las_reader import LASFileReader\n",
    "from data.preprocessor import DataPreprocessor\n",
    "from data.dataset_builder import DatasetBuilder\n",
    "from visualization.scientific_plotter import ScientificPlotter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÅ Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding LAS File Structure\n",
    "\n",
    "Log ASCII Standard (LAS) files are the industry standard for storing well log data. Let's examine the structure and learn how to parse them effectively.\n",
    "\n",
    "### LAS File Sections\n",
    "1. **~V (Version)**: LAS version information\n",
    "2. **~W (Well)**: Well identification and location data\n",
    "3. **~C (Curve)**: Curve definitions, units, and descriptions\n",
    "4. **~P (Parameter)**: Additional parameters and constants\n",
    "5. **~A (ASCII)**: The actual log data in columnar format\n",
    "\n",
    "### Key Challenges\n",
    "- Different LAS versions (1.2, 2.0, 3.0)\n",
    "- Varying curve names and units\n",
    "- Missing or corrupted data\n",
    "- Inconsistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our data processing components\n",
    "las_reader = LASFileReader()\n",
    "preprocessor = DataPreprocessor()\n",
    "dataset_builder = DatasetBuilder()\n",
    "plotter = ScientificPlotter()\n",
    "\n",
    "# Set up data directory\n",
    "data_dir = Path('../data')\n",
    "print(f\"üìÇ Looking for LAS files in: {data_dir}\")\n",
    "\n",
    "# Find all LAS files\n",
    "las_files = list(data_dir.glob('*.las'))\n",
    "print(f\"üîç Found {len(las_files)} LAS files\")\n",
    "\n",
    "if len(las_files) > 0:\n",
    "    print(f\"üìÑ First few files: {[f.name for f in las_files[:5]]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No LAS files found. Please check the data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading and Parsing LAS Files\n",
    "\n",
    "Let's start by examining a single LAS file to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample LAS file\n",
    "if len(las_files) > 0:\n",
    "    sample_file = las_files[0]\n",
    "    print(f\"üìñ Reading sample file: {sample_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the LAS file\n",
    "        well_data = las_reader.read_las_file(str(sample_file))\n",
    "        \n",
    "        print(f\"\\nüìä Well Information:\")\n",
    "        print(f\"  Well ID: {well_data.well_id}\")\n",
    "        print(f\"  Location: {well_data.metadata.location if well_data.metadata else 'Unknown'}\")\n",
    "        print(f\"  Depth range: {well_data.depth.min():.1f} - {well_data.depth.max():.1f} ft\")\n",
    "        print(f\"  Number of depth points: {len(well_data.depth)}\")\n",
    "        \n",
    "        print(f\"\\nüìà Available Curves:\")\n",
    "        for curve_name, curve_data in well_data.curves.items():\n",
    "            valid_points = np.sum(~np.isnan(curve_data))\n",
    "            print(f\"  {curve_name:12s}: {valid_points:4d} valid points ({valid_points/len(curve_data)*100:.1f}%)\")\n",
    "        \n",
    "        # Extract curves for analysis\n",
    "        curves = las_reader.extract_curves(well_data)\n",
    "        print(f\"\\n‚úÖ Successfully extracted {len(curves)} curves\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading LAS file: {e}\")\n",
    "        # Create synthetic data for demonstration\n",
    "        print(\"üîß Creating synthetic data for demonstration...\")\n",
    "        \n",
    "        depth = np.linspace(2000, 2500, 500)\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        curves = {\n",
    "            'GR': 30 + 50 * np.sin(0.01 * depth) + 10 * np.random.randn(len(depth)),\n",
    "            'RHOB': 2.3 + 0.3 * np.random.randn(len(depth)),\n",
    "            'NPHI': 0.15 + 0.1 * np.random.randn(len(depth)),\n",
    "            'RT': 10 * np.exp(np.random.randn(len(depth))),\n",
    "            'PHIE': 0.2 + 0.05 * np.random.randn(len(depth))\n",
    "        }\n",
    "        \n",
    "        # Add some missing values to simulate real data\n",
    "        for curve_name in curves:\n",
    "            missing_idx = np.random.choice(len(depth), size=int(0.05 * len(depth)), replace=False)\n",
    "            curves[curve_name][missing_idx] = np.nan\n",
    "        \n",
    "        print(\"‚úÖ Synthetic data created successfully\")\n",
    "\n",
    "else:\n",
    "    print(\"üîß No LAS files available. Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    depth = np.linspace(2000, 2500, 500)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    curves = {\n",
    "        'GR': 30 + 50 * np.sin(0.01 * depth) + 10 * np.random.randn(len(depth)),\n",
    "        'RHOB': 2.3 + 0.3 * np.random.randn(len(depth)),\n",
    "        'NPHI': 0.15 + 0.1 * np.random.randn(len(depth)),\n",
    "        'RT': 10 * np.exp(np.random.randn(len(depth))),\n",
    "        'PHIE': 0.2 + 0.05 * np.random.randn(len(depth))\n",
    "    }\n",
    "    \n",
    "    # Add some missing values\n",
    "    for curve_name in curves:\n",
    "        missing_idx = np.random.choice(len(depth), size=int(0.05 * len(depth)), replace=False)\n",
    "        curves[curve_name][missing_idx] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Before using well log data for PINN training, we need to assess its quality and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(curves, depth):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    print(\"üîç Data Quality Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    for curve_name, curve_data in curves.items():\n",
    "        # Basic statistics\n",
    "        valid_mask = ~np.isnan(curve_data)\n",
    "        valid_data = curve_data[valid_mask]\n",
    "        \n",
    "        if len(valid_data) == 0:\n",
    "            print(f\"‚ùå {curve_name}: No valid data\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate quality metrics\n",
    "        completeness = len(valid_data) / len(curve_data) * 100\n",
    "        mean_val = np.mean(valid_data)\n",
    "        std_val = np.std(valid_data)\n",
    "        min_val = np.min(valid_data)\n",
    "        max_val = np.max(valid_data)\n",
    "        \n",
    "        # Outlier detection (3-sigma rule)\n",
    "        outliers = np.abs(valid_data - mean_val) > 3 * std_val\n",
    "        outlier_pct = np.sum(outliers) / len(valid_data) * 100\n",
    "        \n",
    "        # Continuity assessment (gaps)\n",
    "        gaps = np.diff(np.where(valid_mask)[0]) > 1\n",
    "        num_gaps = np.sum(gaps)\n",
    "        \n",
    "        quality_report[curve_name] = {\n",
    "            'completeness': completeness,\n",
    "            'mean': mean_val,\n",
    "            'std': std_val,\n",
    "            'range': (min_val, max_val),\n",
    "            'outliers': outlier_pct,\n",
    "            'gaps': num_gaps\n",
    "        }\n",
    "        \n",
    "        # Quality assessment\n",
    "        quality_score = completeness\n",
    "        if outlier_pct > 5:\n",
    "            quality_score -= 10\n",
    "        if num_gaps > 5:\n",
    "            quality_score -= 10\n",
    "        \n",
    "        status = \"‚úÖ\" if quality_score > 80 else \"‚ö†Ô∏è\" if quality_score > 60 else \"‚ùå\"\n",
    "        \n",
    "        print(f\"{status} {curve_name:8s}: {completeness:5.1f}% complete, \"\n",
    "              f\"{outlier_pct:4.1f}% outliers, {num_gaps:2d} gaps, \"\n",
    "              f\"range: [{min_val:6.2f}, {max_val:6.2f}]\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Assess quality of our sample data\n",
    "quality_report = assess_data_quality(curves, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline\n",
    "\n",
    "Now let's implement a comprehensive preprocessing pipeline to clean and standardize our well log data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline\n",
    "print(\"üîß Applying Data Preprocessing Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Step 1: Clean data (remove outliers, handle missing values)\n",
    "print(\"1Ô∏è‚É£ Cleaning data...\")\n",
    "cleaned_curves = preprocessor.clean_data(curves)\n",
    "\n",
    "# Step 2: Normalize curves\n",
    "print(\"2Ô∏è‚É£ Normalizing curves...\")\n",
    "normalized_curves = preprocessor.normalize_curves(cleaned_curves)\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "print(\"3Ô∏è‚É£ Handling missing values...\")\n",
    "final_curves = preprocessor.handle_missing_values(normalized_curves)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# Compare before and after\n",
    "print(\"\\nüìä Preprocessing Results:\")\n",
    "for curve_name in curves.keys():\n",
    "    original_valid = np.sum(~np.isnan(curves[curve_name]))\n",
    "    final_valid = np.sum(~np.isnan(final_curves[curve_name]))\n",
    "    \n",
    "    print(f\"{curve_name:8s}: {original_valid:3d} ‚Üí {final_valid:3d} valid points \"\n",
    "          f\"({final_valid/len(depth)*100:.1f}% complete)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization and Distribution Analysis\n",
    "\n",
    "Visualizing our data helps us understand its characteristics and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data visualization\n",
    "def create_data_visualization(original_curves, processed_curves, depth):\n",
    "    \"\"\"Create comprehensive visualization of well log data\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Create subplot layout\n",
    "    gs = fig.add_gridspec(3, 6, hspace=0.3, wspace=0.4)\n",
    "    \n",
    "    # 1. Well log tracks (original data)\n",
    "    ax_logs = fig.add_subplot(gs[:, :2])\n",
    "    \n",
    "    curve_names = list(original_curves.keys())\n",
    "    colors = ['green', 'blue', 'red', 'purple', 'orange']\n",
    "    \n",
    "    for i, (curve_name, color) in enumerate(zip(curve_names, colors)):\n",
    "        # Normalize for display\n",
    "        curve_data = original_curves[curve_name]\n",
    "        valid_mask = ~np.isnan(curve_data)\n",
    "        \n",
    "        if np.sum(valid_mask) > 0:\n",
    "            norm_data = (curve_data - np.nanmin(curve_data)) / (np.nanmax(curve_data) - np.nanmin(curve_data))\n",
    "            ax_logs.plot(norm_data + i, depth, color=color, linewidth=1, label=curve_name)\n",
    "    \n",
    "    ax_logs.set_ylabel('Depth (ft)')\n",
    "    ax_logs.set_xlabel('Normalized Curve Values')\n",
    "    ax_logs.set_title('Well Log Display (Original Data)')\n",
    "    ax_logs.invert_yaxis()\n",
    "    ax_logs.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax_logs.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distribution plots (top row)\n",
    "    for i, curve_name in enumerate(curve_names[:3]):\n",
    "        ax = fig.add_subplot(gs[0, i+2])\n",
    "        \n",
    "        # Original data\n",
    "        orig_data = original_curves[curve_name]\n",
    "        orig_valid = orig_data[~np.isnan(orig_data)]\n",
    "        \n",
    "        # Processed data\n",
    "        proc_data = processed_curves[curve_name]\n",
    "        proc_valid = proc_data[~np.isnan(proc_data)]\n",
    "        \n",
    "        if len(orig_valid) > 0 and len(proc_valid) > 0:\n",
    "            ax.hist(orig_valid, bins=30, alpha=0.5, color='red', label='Original', density=True)\n",
    "            ax.hist(proc_valid, bins=30, alpha=0.7, color='blue', label='Processed', density=True)\n",
    "            ax.set_title(f'{curve_name} Distribution')\n",
    "            ax.set_xlabel('Value')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation matrix (middle)\n",
    "    ax_corr = fig.add_subplot(gs[1, 2:4])\n",
    "    \n",
    "    # Create correlation matrix from processed data\n",
    "    corr_data = {}\n",
    "    for name, data in processed_curves.items():\n",
    "        valid_mask = ~np.isnan(data)\n",
    "        if np.sum(valid_mask) > 10:  # Need enough points\n",
    "            corr_data[name] = data[valid_mask]\n",
    "    \n",
    "    if len(corr_data) > 1:\n",
    "        # Find common valid indices\n",
    "        min_length = min(len(data) for data in corr_data.values())\n",
    "        corr_matrix_data = {name: data[:min_length] for name, data in corr_data.items()}\n",
    "        \n",
    "        df_corr = pd.DataFrame(corr_matrix_data)\n",
    "        corr_matrix = df_corr.corr()\n",
    "        \n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "                   square=True, ax=ax_corr, cbar_kws={'shrink': 0.8})\n",
    "        ax_corr.set_title('Curve Correlation Matrix')\n",
    "    \n",
    "    # 4. Data completeness (middle right)\n",
    "    ax_complete = fig.add_subplot(gs[1, 4:])\n",
    "    \n",
    "    completeness_orig = []\n",
    "    completeness_proc = []\n",
    "    curve_labels = []\n",
    "    \n",
    "    for name in curve_names:\n",
    "        orig_complete = np.sum(~np.isnan(original_curves[name])) / len(original_curves[name]) * 100\n",
    "        proc_complete = np.sum(~np.isnan(processed_curves[name])) / len(processed_curves[name]) * 100\n",
    "        \n",
    "        completeness_orig.append(orig_complete)\n",
    "        completeness_proc.append(proc_complete)\n",
    "        curve_labels.append(name)\n",
    "    \n",
    "    x = np.arange(len(curve_labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax_complete.bar(x - width/2, completeness_orig, width, label='Original', color='red', alpha=0.7)\n",
    "    ax_complete.bar(x + width/2, completeness_proc, width, label='Processed', color='blue', alpha=0.7)\n",
    "    \n",
    "    ax_complete.set_xlabel('Curves')\n",
    "    ax_complete.set_ylabel('Completeness (%)')\n",
    "    ax_complete.set_title('Data Completeness Comparison')\n",
    "    ax_complete.set_xticks(x)\n",
    "    ax_complete.set_xticklabels(curve_labels, rotation=45)\n",
    "    ax_complete.legend()\n",
    "    ax_complete.grid(True, alpha=0.3)\n",
    "    ax_complete.set_ylim(0, 105)\n",
    "    \n",
    "    # 5. Quality metrics (bottom)\n",
    "    ax_quality = fig.add_subplot(gs[2, 2:])\n",
    "    \n",
    "    # Calculate quality scores\n",
    "    quality_scores = []\n",
    "    for name in curve_names:\n",
    "        data = processed_curves[name]\n",
    "        valid_data = data[~np.isnan(data)]\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            completeness = len(valid_data) / len(data) * 100\n",
    "            # Simple quality score based on completeness and variance\n",
    "            cv = np.std(valid_data) / np.abs(np.mean(valid_data)) if np.mean(valid_data) != 0 else 0\n",
    "            quality = completeness * (1 - min(cv, 1))  # Penalize high coefficient of variation\n",
    "            quality_scores.append(max(0, quality))\n",
    "        else:\n",
    "            quality_scores.append(0)\n",
    "    \n",
    "    bars = ax_quality.bar(curve_labels, quality_scores, color=['green' if q > 70 else 'orange' if q > 40 else 'red' for q in quality_scores])\n",
    "    ax_quality.set_xlabel('Curves')\n",
    "    ax_quality.set_ylabel('Quality Score')\n",
    "    ax_quality.set_title('Data Quality Assessment')\n",
    "    ax_quality.set_ylim(0, 100)\n",
    "    \n",
    "    # Add quality score labels\n",
    "    for bar, score in zip(bars, quality_scores):\n",
    "        height = bar.get_height()\n",
    "        ax_quality.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                       f'{score:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax_quality.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Well Log Data Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "create_data_visualization(curves, final_curves, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Well Data Processing\n",
    "\n",
    "For PINN training, we typically need data from multiple wells. Let's process multiple LAS files and create a comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_wells(las_files, max_wells=10):\n",
    "    \"\"\"Process multiple LAS files and create a combined dataset\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Processing up to {max_wells} wells...\")\n",
    "    \n",
    "    processed_wells = []\n",
    "    processing_stats = {\n",
    "        'total_files': len(las_files),\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'filtered_out': 0\n",
    "    }\n",
    "    \n",
    "    for i, las_file in enumerate(las_files[:max_wells]):\n",
    "        try:\n",
    "            print(f\"  üìÑ Processing {las_file.name}... \", end=\"\")\n",
    "            \n",
    "            # Read LAS file\n",
    "            well_data = las_reader.read_las_file(str(las_file))\n",
    "            \n",
    "            # Extract curves\n",
    "            curves = las_reader.extract_curves(well_data)\n",
    "            \n",
    "            # Check if well has required curves\n",
    "            required_curves = ['GR', 'PHIE', 'PERM']  # Minimum required\n",
    "            has_required = all(curve in curves for curve in required_curves)\n",
    "            \n",
    "            if not has_required:\n",
    "                print(\"‚ùå Missing required curves\")\n",
    "                processing_stats['filtered_out'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            cleaned_curves = preprocessor.clean_data(curves)\n",
    "            normalized_curves = preprocessor.normalize_curves(cleaned_curves)\n",
    "            final_curves = preprocessor.handle_missing_values(normalized_curves)\n",
    "            \n",
    "            # Check data quality\n",
    "            total_points = len(well_data.depth)\n",
    "            valid_points = sum(np.sum(~np.isnan(curve)) for curve in final_curves.values())\n",
    "            quality_ratio = valid_points / (total_points * len(final_curves))\n",
    "            \n",
    "            if quality_ratio < 0.7:  # Require 70% data completeness\n",
    "                print(f\"‚ùå Low quality ({quality_ratio:.1%})\")\n",
    "                processing_stats['filtered_out'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Store processed well data\n",
    "            processed_well = {\n",
    "                'well_id': well_data.well_id,\n",
    "                'depth': well_data.depth,\n",
    "                'curves': final_curves,\n",
    "                'metadata': well_data.metadata,\n",
    "                'quality_ratio': quality_ratio\n",
    "            }\n",
    "            \n",
    "            processed_wells.append(processed_well)\n",
    "            processing_stats['successful'] += 1\n",
    "            \n",
    "            print(f\"‚úÖ Success ({quality_ratio:.1%} quality)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)[:50]}...\")\n",
    "            processing_stats['failed'] += 1\n",
    "    \n",
    "    return processed_wells, processing_stats\n",
    "\n",
    "# Process multiple wells (or create synthetic data if no files available)\n",
    "if len(las_files) > 0:\n",
    "    processed_wells, stats = process_multiple_wells(las_files, max_wells=5)\n",
    "else:\n",
    "    print(\"üîß Creating synthetic multi-well dataset...\")\n",
    "    \n",
    "    # Create synthetic wells with different characteristics\n",
    "    processed_wells = []\n",
    "    \n",
    "    for well_id in range(5):\n",
    "        np.random.seed(42 + well_id)\n",
    "        \n",
    "        # Vary depth ranges and characteristics\n",
    "        depth_start = 2000 + well_id * 100\n",
    "        depth_end = depth_start + 400 + well_id * 50\n",
    "        depth = np.linspace(depth_start, depth_end, 400)\n",
    "        \n",
    "        # Create curves with well-specific characteristics\n",
    "        base_porosity = 0.15 + well_id * 0.02\n",
    "        base_permeability = 10 * (well_id + 1)\n",
    "        \n",
    "        curves = {\n",
    "            'GR': 40 + 30 * np.sin(0.01 * depth) + 15 * np.random.randn(len(depth)),\n",
    "            'PHIE': base_porosity + 0.05 * np.random.randn(len(depth)),\n",
    "            'PERM': base_permeability * np.exp(0.5 * np.random.randn(len(depth))),\n",
    "            'RT': 5 + 10 * np.exp(np.random.randn(len(depth))),\n",
    "            'RHOB': 2.2 + 0.4 * np.random.randn(len(depth))\n",
    "        }\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        cleaned_curves = preprocessor.clean_data(curves)\n",
    "        normalized_curves = preprocessor.normalize_curves(cleaned_curves)\n",
    "        final_curves = preprocessor.handle_missing_values(normalized_curves)\n",
    "        \n",
    "        processed_well = {\n",
    "            'well_id': f'SYNTHETIC_WELL_{well_id+1:02d}',\n",
    "            'depth': depth,\n",
    "            'curves': final_curves,\n",
    "            'metadata': None,\n",
    "            'quality_ratio': 0.95\n",
    "        }\n",
    "        \n",
    "        processed_wells.append(processed_well)\n",
    "    \n",
    "    stats = {'successful': 5, 'failed': 0, 'filtered_out': 0, 'total_files': 5}\n",
    "\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"  ‚úÖ Successful: {stats['successful']}\")\n",
    "print(f\"  ‚ùå Failed: {stats['failed']}\")\n",
    "print(f\"  üö´ Filtered out: {stats['filtered_out']}\")\n",
    "print(f\"  üìà Success rate: {stats['successful']/stats['total_files']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Creation for PINN Training\n",
    "\n",
    "Now let's create training and validation datasets suitable for PINN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for PINN training\n",
    "print(\"üèóÔ∏è Building PINN Training Datasets\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Build combined dataset\n",
    "training_config = {\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'random_seed': 42,\n",
    "    'min_points_per_well': 100\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "datasets = dataset_builder.create_datasets(processed_wells, training_config)\n",
    "\n",
    "print(f\"üì¶ Dataset Creation Results:\")\n",
    "print(f\"  üèãÔ∏è Training samples: {len(datasets['train']['depth'])}\")\n",
    "print(f\"  üîç Validation samples: {len(datasets['validation']['depth'])}\")\n",
    "print(f\"  üß™ Test samples: {len(datasets['test']['depth'])}\")\n",
    "print(f\"  üìä Total wells used: {len(processed_wells)}\")\n",
    "\n",
    "# Analyze dataset characteristics\n",
    "def analyze_dataset_characteristics(datasets):\n",
    "    \"\"\"Analyze the characteristics of our PINN datasets\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    dataset_names = ['train', 'validation', 'test']\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    \n",
    "    # 1. Depth distribution\n",
    "    for i, (name, color) in enumerate(zip(dataset_names, colors)):\n",
    "        depths = datasets[name]['depth']\n",
    "        axes[0, 0].hist(depths, bins=30, alpha=0.6, label=name.title(), color=color, density=True)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Depth (ft)')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].set_title('Depth Distribution Across Datasets')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Porosity distribution\n",
    "    for i, (name, color) in enumerate(zip(dataset_names, colors)):\n",
    "        porosity = datasets[name]['curves']['PHIE']\n",
    "        valid_porosity = porosity[~np.isnan(porosity)]\n",
    "        if len(valid_porosity) > 0:\n",
    "            axes[0, 1].hist(valid_porosity, bins=30, alpha=0.6, label=name.title(), color=color, density=True)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Porosity (fraction)')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].set_title('Porosity Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Permeability distribution (log scale)\n",
    "    for i, (name, color) in enumerate(zip(dataset_names, colors)):\n",
    "        perm = datasets[name]['curves']['PERM']\n",
    "        valid_perm = perm[~np.isnan(perm)]\n",
    "        if len(valid_perm) > 0 and np.all(valid_perm > 0):\n",
    "            axes[0, 2].hist(np.log10(valid_perm), bins=30, alpha=0.6, label=name.title(), color=color, density=True)\n",
    "    \n",
    "    axes[0, 2].set_xlabel('Log10(Permeability)')\n",
    "    axes[0, 2].set_ylabel('Density')\n",
    "    axes[0, 2].set_title('Permeability Distribution (Log Scale)')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Data completeness by curve\n",
    "    curve_names = list(datasets['train']['curves'].keys())\n",
    "    completeness_data = {name: [] for name in dataset_names}\n",
    "    \n",
    "    for curve in curve_names:\n",
    "        for name in dataset_names:\n",
    "            curve_data = datasets[name]['curves'][curve]\n",
    "            completeness = np.sum(~np.isnan(curve_data)) / len(curve_data) * 100\n",
    "            completeness_data[name].append(completeness)\n",
    "    \n",
    "    x = np.arange(len(curve_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (name, color) in enumerate(zip(dataset_names, colors)):\n",
    "        axes[1, 0].bar(x + i*width, completeness_data[name], width, label=name.title(), color=color, alpha=0.7)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Curves')\n",
    "    axes[1, 0].set_ylabel('Completeness (%)')\n",
    "    axes[1, 0].set_title('Data Completeness by Curve')\n",
    "    axes[1, 0].set_xticks(x + width)\n",
    "    axes[1, 0].set_xticklabels(curve_names, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Sample distribution by well\n",
    "    well_counts = {name: {} for name in dataset_names}\n",
    "    \n",
    "    for name in dataset_names:\n",
    "        well_ids = datasets[name]['well_ids']\n",
    "        unique_wells, counts = np.unique(well_ids, return_counts=True)\n",
    "        well_counts[name] = dict(zip(unique_wells, counts))\n",
    "    \n",
    "    # Get all unique wells\n",
    "    all_wells = set()\n",
    "    for name in dataset_names:\n",
    "        all_wells.update(well_counts[name].keys())\n",
    "    all_wells = sorted(list(all_wells))\n",
    "    \n",
    "    x = np.arange(len(all_wells))\n",
    "    \n",
    "    for i, (name, color) in enumerate(zip(dataset_names, colors)):\n",
    "        counts = [well_counts[name].get(well, 0) for well in all_wells]\n",
    "        axes[1, 1].bar(x + i*width, counts, width, label=name.title(), color=color, alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Wells')\n",
    "    axes[1, 1].set_ylabel('Number of Samples')\n",
    "    axes[1, 1].set_title('Sample Distribution by Well')\n",
    "    axes[1, 1].set_xticks(x + width)\n",
    "    axes[1, 1].set_xticklabels([w[:10] + '...' if len(w) > 10 else w for w in all_wells], rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Feature correlation in training data\n",
    "    train_curves = datasets['train']['curves']\n",
    "    corr_data = {}\n",
    "    \n",
    "    for name, data in train_curves.items():\n",
    "        valid_mask = ~np.isnan(data)\n",
    "        if np.sum(valid_mask) > 100:\n",
    "            corr_data[name] = data[valid_mask][:1000]  # Limit for correlation\n",
    "    \n",
    "    if len(corr_data) > 1:\n",
    "        min_length = min(len(data) for data in corr_data.values())\n",
    "        corr_matrix_data = {name: data[:min_length] for name, data in corr_data.items()}\n",
    "        \n",
    "        df_corr = pd.DataFrame(corr_matrix_data)\n",
    "        corr_matrix = df_corr.corr()\n",
    "        \n",
    "        im = axes[1, 2].imshow(corr_matrix.values, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        axes[1, 2].set_xticks(range(len(corr_matrix.columns)))\n",
    "        axes[1, 2].set_yticks(range(len(corr_matrix.columns)))\n",
    "        axes[1, 2].set_xticklabels(corr_matrix.columns, rotation=45)\n",
    "        axes[1, 2].set_yticklabels(corr_matrix.columns)\n",
    "        axes[1, 2].set_title('Feature Correlation Matrix\\n(Training Data)')\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i in range(len(corr_matrix)):\n",
    "            for j in range(len(corr_matrix)):\n",
    "                axes[1, 2].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                               ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        plt.colorbar(im, ax=axes[1, 2], shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('PINN Dataset Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Analyze our datasets\n",
    "analyze_dataset_characteristics(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Export and Preparation for PINN Training\n",
    "\n",
    "Finally, let's prepare our processed data for use in PINN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PINN training\n",
    "def prepare_pinn_training_data(datasets):\n",
    "    \"\"\"Prepare final datasets for PINN training\"\"\"\n",
    "    \n",
    "    print(\"üéØ Preparing Data for PINN Training\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    pinn_data = {}\n",
    "    \n",
    "    for split_name in ['train', 'validation', 'test']:\n",
    "        split_data = datasets[split_name]\n",
    "        \n",
    "        # Extract features (inputs to PINN)\n",
    "        features = []\n",
    "        feature_names = ['depth']\n",
    "        \n",
    "        # Add depth as first feature\n",
    "        features.append(split_data['depth'].reshape(-1, 1))\n",
    "        \n",
    "        # Add curve data as features\n",
    "        for curve_name in ['GR', 'PHIE', 'PERM']:\n",
    "            if curve_name in split_data['curves']:\n",
    "                curve_data = split_data['curves'][curve_name]\n",
    "                features.append(curve_data.reshape(-1, 1))\n",
    "                feature_names.append(curve_name)\n",
    "        \n",
    "        # Combine features\n",
    "        X = np.hstack(features)\n",
    "        \n",
    "        # Create targets (what PINN should predict)\n",
    "        # For demonstration, we'll use porosity and a synthetic pressure field\n",
    "        porosity = split_data['curves']['PHIE']\n",
    "        \n",
    "        # Create synthetic pressure field based on depth and porosity\n",
    "        depth_norm = (split_data['depth'] - np.min(split_data['depth'])) / (np.max(split_data['depth']) - np.min(split_data['depth']))\n",
    "        pressure = 100 + 50 * depth_norm + 20 * (1 - porosity) + 5 * np.random.randn(len(porosity))\n",
    "        \n",
    "        # Create synthetic saturation field\n",
    "        saturation = 0.3 + 0.4 * porosity + 0.1 * np.random.randn(len(porosity))\n",
    "        saturation = np.clip(saturation, 0.2, 0.8)\n",
    "        \n",
    "        y = np.column_stack([pressure, saturation])\n",
    "        target_names = ['pressure', 'saturation']\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.any(np.isnan(X), axis=1) & ~np.any(np.isnan(y), axis=1)\n",
    "        \n",
    "        X_clean = X[valid_mask]\n",
    "        y_clean = y[valid_mask]\n",
    "        well_ids_clean = np.array(split_data['well_ids'])[valid_mask]\n",
    "        \n",
    "        pinn_data[split_name] = {\n",
    "            'X': X_clean,\n",
    "            'y': y_clean,\n",
    "            'well_ids': well_ids_clean,\n",
    "            'feature_names': feature_names,\n",
    "            'target_names': target_names,\n",
    "            'n_samples': len(X_clean)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {split_name:10s}: {len(X_clean):5d} samples, {X_clean.shape[1]} features, {y_clean.shape[1]} targets\")\n",
    "    \n",
    "    return pinn_data\n",
    "\n",
    "# Prepare PINN training data\n",
    "pinn_data = prepare_pinn_training_data(datasets)\n",
    "\n",
    "# Display data summary\n",
    "print(f\"\\nüìã PINN Data Summary:\")\n",
    "print(f\"  Features: {pinn_data['train']['feature_names']}\")\n",
    "print(f\"  Targets: {pinn_data['train']['target_names']}\")\n",
    "print(f\"  Feature ranges:\")\n",
    "\n",
    "X_train = pinn_data['train']['X']\n",
    "for i, name in enumerate(pinn_data['train']['feature_names']):\n",
    "    min_val, max_val = np.min(X_train[:, i]), np.max(X_train[:, i])\n",
    "    print(f\"    {name:8s}: [{min_val:8.3f}, {max_val:8.3f}]\")\n",
    "\n",
    "print(f\"  Target ranges:\")\n",
    "y_train = pinn_data['train']['y']\n",
    "for i, name in enumerate(pinn_data['train']['target_names']):\n",
    "    min_val, max_val = np.min(y_train[:, i]), np.max(y_train[:, i])\n",
    "    print(f\"    {name:10s}: [{min_val:8.3f}, {max_val:8.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "In this tutorial, we've covered the complete data processing pipeline for PINN training with well log data.\n",
    "\n",
    "### Key Accomplishments\n",
    "1. **LAS File Processing**: Successfully read and parsed LAS files with robust error handling\n",
    "2. **Data Quality Assessment**: Implemented comprehensive quality metrics and filtering\n",
    "3. **Preprocessing Pipeline**: Applied cleaning, normalization, and missing value handling\n",
    "4. **Multi-Well Integration**: Combined data from multiple wells into coherent datasets\n",
    "5. **PINN-Ready Datasets**: Created training/validation/test splits suitable for physics-informed learning\n",
    "\n",
    "### Best Practices for Well Log Data Processing\n",
    "\n",
    "#### Data Quality\n",
    "- **Completeness**: Require at least 70% data completeness for reliable training\n",
    "- **Consistency**: Standardize curve names and units across wells\n",
    "- **Outlier Detection**: Use statistical methods (3-sigma rule) to identify anomalous values\n",
    "- **Gap Analysis**: Monitor data continuity and interpolate small gaps carefully\n",
    "\n",
    "#### Preprocessing Strategy\n",
    "- **Normalization**: Essential for neural network training stability\n",
    "- **Missing Values**: Use domain-appropriate interpolation methods\n",
    "- **Feature Engineering**: Consider derived properties (porosity-permeability relationships)\n",
    "- **Validation**: Always validate preprocessing results visually and statistically\n",
    "\n",
    "#### Dataset Design\n",
    "- **Stratified Splitting**: Ensure representative samples in train/validation/test sets\n",
    "- **Well-Based Splitting**: Avoid data leakage by splitting at well level\n",
    "- **Balanced Representation**: Include diverse geological conditions\n",
    "- **Physics Constraints**: Ensure data respects known physical relationships\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "1. **Ignoring Data Quality**: Poor quality data leads to poor PINN performance\n",
    "2. **Over-Preprocessing**: Excessive smoothing can remove important physics\n",
    "3. **Data Leakage**: Using future information or mixing wells inappropriately\n",
    "4. **Scale Issues**: Forgetting to normalize features with very different ranges\n",
    "5. **Physics Violations**: Creating datasets that violate known physical constraints\n",
    "\n",
    "### Next Steps\n",
    "With our processed datasets ready, we can now move to:\n",
    "1. **Tutorial 3**: Implement PINN architecture with physics constraints\n",
    "2. **Tutorial 4**: Train PINNs with optimization best practices\n",
    "3. **Tutorial 5**: Validate results and analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for next tutorials\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save PINN-ready datasets\n",
    "with open(output_dir / 'pinn_datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(pinn_data, f)\n",
    "\n",
    "print(\"üíæ Datasets saved successfully!\")\n",
    "print(f\"üìÅ Location: {output_dir / 'pinn_datasets.pkl'}\")\n",
    "print(\"\\nüéâ Tutorial 2 Complete!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Tutorial 3 - PINN Model Implementation\")\n",
    "\n",
    "# Quick verification\n",
    "print(f\"\\nüîç Final Verification:\")\n",
    "print(f\"  Training samples: {pinn_data['train']['n_samples']:,}\")\n",
    "print(f\"  Validation samples: {pinn_data['validation']['n_samples']:,}\")\n",
    "print(f\"  Test samples: {pinn_data['test']['n_samples']:,}\")\n",
    "print(f\"  Features: {len(pinn_data['train']['feature_names'])}\")\n",
    "print(f\"  Targets: {len(pinn_data['train']['target_names'])}\")\n",
    "print(f\"  Wells processed: {len(processed_wells)}\")\n",
    "print(\"\\n‚úÖ Ready for PINN training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}