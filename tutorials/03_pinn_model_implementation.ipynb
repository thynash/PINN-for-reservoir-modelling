{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks for Reservoir Modeling\n",
    "## Tutorial 3: PINN Model Implementation\n",
    "\n",
    "In this tutorial, we'll implement a complete Physics-Informed Neural Network (PINN) for reservoir modeling. We'll build the neural network architecture, implement physics constraints, and demonstrate how automatic differentiation enables physics-informed learning.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Implement a PINN architecture suitable for reservoir modeling\n",
    "- Integrate physics constraints (Darcy's law, continuity equation) into the loss function\n",
    "- Use automatic differentiation to compute PDE residuals\n",
    "- Handle boundary conditions and initial conditions\n",
    "- Combine data loss with physics loss for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.pinn_architecture import PINNArchitecture\n",
    "from models.tensor_manager import TensorManager\n",
    "from models.model_interface import ModelInterface\n",
    "from physics.pde_formulator import PDEFormulator\n",
    "from physics.boundary_conditions import BoundaryConditionHandler\n",
    "from physics.physics_loss import PhysicsLossCalculator\n",
    "from visualization.scientific_plotter import ScientificPlotter\n",
    "\n",
    "# Set up device and plotting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "First, let's load the processed well log data from Tutorial 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "data_path = Path('../output/pinn_datasets.pkl')\n",
    "\n",
    "if data_path.exists():\n",
    "    with open(data_path, 'rb') as f:\n",
    "        pinn_data = pickle.load(f)\n",
    "    print(\"üìÇ Loaded processed datasets from Tutorial 2\")\nelse:\n",
    "    print(\"üîß Creating synthetic datasets for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data similar to Tutorial 2\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_train = 2000\n",
    "    n_val = 500\n",
    "    n_test = 300\n",
    "    \n",
    "    def create_synthetic_data(n_samples):\n",
    "        # Features: depth, GR, PHIE, PERM\n",
    "        depth = np.random.uniform(2000, 2500, n_samples)\n",
    "        gr = 30 + 50 * np.sin(0.01 * depth) + 10 * np.random.randn(n_samples)\n",
    "        phie = 0.15 + 0.05 * np.random.randn(n_samples)\n",
    "        perm = 10 * np.exp(2 * (phie - 0.15)) * np.exp(0.5 * np.random.randn(n_samples))\n",
    "        \n",
    "        X = np.column_stack([depth, gr, phie, perm])\n",
    "        \n",
    "        # Targets: pressure, saturation\n",
    "        depth_norm = (depth - 2000) / 500\n",
    "        pressure = 100 + 50 * depth_norm + 20 * (1 - phie) + 5 * np.random.randn(n_samples)\n",
    "        saturation = 0.3 + 0.4 * phie + 0.1 * np.random.randn(n_samples)\n",
    "        saturation = np.clip(saturation, 0.2, 0.8)\n",
    "        \n",
    "        y = np.column_stack([pressure, saturation])\n",
    "        \n",
    "        well_ids = [f'WELL_{i//100:02d}' for i in range(n_samples)]\n",
    "        \n",
    "        return {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'well_ids': well_ids,\n",
    "            'feature_names': ['depth', 'GR', 'PHIE', 'PERM'],\n",
    "            'target_names': ['pressure', 'saturation'],\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "    \n",
    "    pinn_data = {\n",
    "        'train': create_synthetic_data(n_train),\n",
    "        'validation': create_synthetic_data(n_val),\n",
    "        'test': create_synthetic_data(n_test)\n",
    "    }\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    data = pinn_data[split]\n",
    "    print(f\"  {split:10s}: {data['n_samples']:4d} samples, {data['X'].shape[1]} features, {data['y'].shape[1]} targets\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Features: {pinn_data['train']['feature_names']}\")\n",
    "print(f\"üéØ Targets: {pinn_data['train']['target_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PINN Architecture Design\n",
    "\n",
    "Let's design and implement our PINN architecture. The network will take reservoir properties as input and predict pressure and saturation fields.\n",
    "\n",
    "### Architecture Components\n",
    "1. **Input Layer**: Depth, gamma ray, porosity, permeability\n",
    "2. **Hidden Layers**: Multiple fully connected layers with smooth activations\n",
    "3. **Output Layer**: Pressure and saturation predictions\n",
    "4. **Physics Integration**: Automatic differentiation for PDE residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PINN components\n",
    "print(\"üèóÔ∏è Building PINN Architecture\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'input_dim': 4,  # depth, GR, PHIE, PERM\n",
    "    'hidden_dims': [64, 64, 64],  # 3 hidden layers with 64 neurons each\n",
    "    'output_dim': 2,  # pressure, saturation\n",
    "    'activation': 'tanh',  # Smooth activation for physics\n",
    "    'dropout_rate': 0.0  # No dropout for physics problems\n",
    "}\n",
    "\n",
    "# Create PINN model\n",
    "pinn_model = PINNArchitecture(\n",
    "    input_dim=model_config['input_dim'],\n",
    "    hidden_dims=model_config['hidden_dims'],\n",
    "    output_dim=model_config['output_dim'],\n",
    "    activation=model_config['activation']\n",
    ").to(device)\n",
    "\n",
    "# Initialize tensor manager and model interface\n",
    "tensor_manager = TensorManager(device=device)\n",
    "model_interface = ModelInterface(pinn_model, tensor_manager)\n",
    "\n",
    "print(f\"‚úÖ PINN Model Created:\")\n",
    "print(f\"  Input dimension: {model_config['input_dim']}\")\n",
    "print(f\"  Hidden layers: {model_config['hidden_dims']}\")\n",
    "print(f\"  Output dimension: {model_config['output_dim']}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in pinn_model.parameters()):,}\")\n",
    "print(f\"  Activation: {model_config['activation']}\")\n",
    "\n",
    "# Display model architecture\n",
    "print(f\"\\nüèõÔ∏è Model Architecture:\")\n",
    "print(pinn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Physics Formulation\n",
    "\n",
    "Now let's implement the physics constraints that will guide our PINN training. We'll focus on the fundamental equations governing fluid flow in porous media.\n",
    "\n",
    "### Governing Equations\n",
    "\n",
    "1. **Darcy's Law**: $\\vec{v} = -\\frac{k}{\\mu} \\nabla p$\n",
    "2. **Continuity Equation**: $\\nabla \\cdot \\vec{v} = 0$\n",
    "3. **Combined**: $\\nabla \\cdot \\left(\\frac{k}{\\mu} \\nabla p\\right) = 0$\n",
    "\n",
    "For two-phase flow:\n",
    "4. **Buckley-Leverett**: $\\frac{\\partial S_w}{\\partial t} + \\frac{\\partial f_w}{\\partial x} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize physics components\n",
    "print(\"‚öóÔ∏è Setting up Physics Constraints\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Create physics components\n",
    "pde_formulator = PDEFormulator()\n",
    "boundary_handler = BoundaryConditionHandler()\n",
    "physics_calculator = PhysicsLossCalculator(pde_formulator, boundary_handler)\n",
    "\n",
    "print(\"‚úÖ Physics components initialized\")\n",
    "\n",
    "# Demonstrate physics constraint calculation\n",
    "def demonstrate_physics_constraints():\n",
    "    \"\"\"Demonstrate how physics constraints are computed\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Physics Constraint Demonstration\")\n",
    "    \n",
    "    # Create sample input data\n",
    "    batch_size = 100\n",
    "    sample_input = torch.randn(batch_size, 4, requires_grad=True, device=device)\n",
    "    \n",
    "    # Forward pass through PINN\n",
    "    with torch.enable_grad():\n",
    "        predictions = pinn_model(sample_input)\n",
    "        pressure = predictions[:, 0:1]  # First output\n",
    "        saturation = predictions[:, 1:2]  # Second output\n",
    "    \n",
    "    print(f\"  üìä Sample batch: {batch_size} points\")\n",
    "    print(f\"  üîÆ Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Compute physics residuals\n",
    "    try:\n",
    "        # Darcy residual (simplified for demonstration)\n",
    "        coords = sample_input[:, :2]  # Use first 2 features as coordinates\n",
    "        permeability = sample_input[:, 3:4]  # Permeability from input\n",
    "        \n",
    "        darcy_residual = pde_formulator.darcy_residual(pressure, coords, permeability)\n",
    "        \n",
    "        print(f\"  ‚öóÔ∏è Darcy residual computed: {darcy_residual.shape}\")\n",
    "        print(f\"     Mean residual: {torch.mean(torch.abs(darcy_residual)).item():.6f}\")\n",
    "        print(f\"     Max residual: {torch.max(torch.abs(darcy_residual)).item():.6f}\")\n",
    "        \n",
    "        # Boundary conditions (example)\n",
    "        boundary_points = sample_input[:10]  # First 10 points as boundary\n",
    "        boundary_values = torch.ones(10, 1, device=device) * 100  # Fixed pressure\n",
    "        \n",
    "        boundary_residual = boundary_handler.dirichlet_bc(\n",
    "            pinn_model(boundary_points)[:, 0:1], boundary_values\n",
    "        )\n",
    "        \n",
    "        print(f\"  üöß Boundary residual: {boundary_residual.shape}\")\n",
    "        print(f\"     Mean BC violation: {torch.mean(torch.abs(boundary_residual)).item():.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Physics computation demo: {str(e)[:50]}...\")\n",
    "        print(\"     (This is expected for untrained model)\")\n",
    "\n",
    "demonstrate_physics_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automatic Differentiation Deep Dive\n",
    "\n",
    "The key to PINNs is automatic differentiation. Let's explore how PyTorch computes gradients and how we use them for physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_automatic_differentiation():\n",
    "    \"\"\"Comprehensive demonstration of automatic differentiation for PINNs\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Automatic Differentiation Deep Dive\")\n",
    "    print(\"=\" * 38)\n",
    "    \n",
    "    # Create a simple 2D example for visualization\n",
    "    x = torch.linspace(0, 1, 50, requires_grad=True, device=device)\n",
    "    y = torch.linspace(0, 1, 50, requires_grad=True, device=device)\n",
    "    \n",
    "    # Create meshgrid\n",
    "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "    coords = torch.stack([X.flatten(), Y.flatten()], dim=1)\n",
    "    coords.requires_grad_(True)\n",
    "    \n",
    "    # Add dummy features to match model input\n",
    "    dummy_features = torch.zeros(coords.shape[0], 2, device=device)\n",
    "    model_input = torch.cat([coords, dummy_features], dim=1)\n",
    "    \n",
    "    print(f\"üìê Grid setup: {len(x)}√ó{len(y)} = {coords.shape[0]} points\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.enable_grad():\n",
    "        output = pinn_model(model_input)\n",
    "        u = output[:, 0]  # Pressure field\n",
    "        v = output[:, 1]  # Saturation field\n",
    "    \n",
    "    print(f\"üîÆ Forward pass complete: {output.shape}\")\n",
    "    \n",
    "    # Compute first derivatives\n",
    "    print(\"\\nüìä Computing Derivatives:\")\n",
    "    \n",
    "    # First derivatives of pressure\n",
    "    u_grad = torch.autograd.grad(\n",
    "        outputs=u.sum(), \n",
    "        inputs=coords, \n",
    "        create_graph=True, \n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    u_x = u_grad[:, 0]  # ‚àÇu/‚àÇx\n",
    "    u_y = u_grad[:, 1]  # ‚àÇu/‚àÇy\n",
    "    \n",
    "    print(f\"  ‚àÇu/‚àÇx computed: {u_x.shape}\")\n",
    "    print(f\"  ‚àÇu/‚àÇy computed: {u_y.shape}\")\n",
    "    \n",
    "    # Second derivatives (for Laplacian)\n",
    "    u_xx = torch.autograd.grad(\n",
    "        outputs=u_x.sum(), \n",
    "        inputs=coords, \n",
    "        create_graph=True, \n",
    "        retain_graph=True\n",
    "    )[0][:, 0]\n",
    "    \n",
    "    u_yy = torch.autograd.grad(\n",
    "        outputs=u_y.sum(), \n",
    "        inputs=coords, \n",
    "        create_graph=True, \n",
    "        retain_graph=True\n",
    "    )[0][:, 1]\n",
    "    \n",
    "    # Laplacian: ‚àá¬≤u = ‚àÇ¬≤u/‚àÇx¬≤ + ‚àÇ¬≤u/‚àÇy¬≤\n",
    "    laplacian = u_xx + u_yy\n",
    "    \n",
    "    print(f\"  ‚àÇ¬≤u/‚àÇx¬≤ computed: {u_xx.shape}\")\n",
    "    print(f\"  ‚àÇ¬≤u/‚àÇy¬≤ computed: {u_yy.shape}\")\n",
    "    print(f\"  ‚àá¬≤u (Laplacian): {laplacian.shape}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìà Derivative Statistics:\")\n",
    "    print(f\"  |‚àÇu/‚àÇx| mean: {torch.mean(torch.abs(u_x)).item():.6f}\")\n",
    "    print(f\"  |‚àÇu/‚àÇy| mean: {torch.mean(torch.abs(u_y)).item():.6f}\")\n",
    "    print(f\"  |‚àá¬≤u| mean: {torch.mean(torch.abs(laplacian)).item():.6f}\")\n",
    "    print(f\"  |‚àá¬≤u| max: {torch.max(torch.abs(laplacian)).item():.6f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    u_np = u.detach().cpu().numpy().reshape(50, 50)\n",
    "    v_np = v.detach().cpu().numpy().reshape(50, 50)\n",
    "    u_x_np = u_x.detach().cpu().numpy().reshape(50, 50)\n",
    "    u_y_np = u_y.detach().cpu().numpy().reshape(50, 50)\n",
    "    u_xx_np = u_xx.detach().cpu().numpy().reshape(50, 50)\n",
    "    laplacian_np = laplacian.detach().cpu().numpy().reshape(50, 50)\n",
    "    \n",
    "    # Plot fields\n",
    "    im1 = axes[0, 0].imshow(u_np, extent=[0, 1, 0, 1], origin='lower', cmap='viridis')\n",
    "    axes[0, 0].set_title('Pressure Field u(x,y)')\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('y')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    im2 = axes[0, 1].imshow(v_np, extent=[0, 1, 0, 1], origin='lower', cmap='plasma')\n",
    "    axes[0, 1].set_title('Saturation Field v(x,y)')\n",
    "    axes[0, 1].set_xlabel('x')\n",
    "    axes[0, 1].set_ylabel('y')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    im3 = axes[0, 2].imshow(u_x_np, extent=[0, 1, 0, 1], origin='lower', cmap='RdBu')\n",
    "    axes[0, 2].set_title('‚àÇu/‚àÇx')\n",
    "    axes[0, 2].set_xlabel('x')\n",
    "    axes[0, 2].set_ylabel('y')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    im4 = axes[1, 0].imshow(u_y_np, extent=[0, 1, 0, 1], origin='lower', cmap='RdBu')\n",
    "    axes[1, 0].set_title('‚àÇu/‚àÇy')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('y')\n",
    "    plt.colorbar(im4, ax=axes[1, 0])\n",
    "    \n",
    "    im5 = axes[1, 1].imshow(u_xx_np, extent=[0, 1, 0, 1], origin='lower', cmap='seismic')\n",
    "    axes[1, 1].set_title('‚àÇ¬≤u/‚àÇx¬≤')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('y')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    im6 = axes[1, 2].imshow(laplacian_np, extent=[0, 1, 0, 1], origin='lower', cmap='seismic')\n",
    "    axes[1, 2].set_title('‚àá¬≤u (Laplacian)')\n",
    "    axes[1, 2].set_xlabel('x')\n",
    "    axes[1, 2].set_ylabel('y')\n",
    "    plt.colorbar(im6, ax=axes[1, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Automatic Differentiation in PINNs', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'u': u,\n",
    "        'u_x': u_x,\n",
    "        'u_y': u_y,\n",
    "        'laplacian': laplacian\n",
    "    }\n",
    "\n",
    "# Run the demonstration\n",
    "autodiff_results = demonstrate_automatic_differentiation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete PINN Loss Function\n",
    "\n",
    "Now let's implement the complete PINN loss function that combines data fitting with physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletePINNLoss(nn.Module):\n",
    "    \"\"\"Complete PINN loss function combining data and physics terms\"\"\"\n",
    "    \n",
    "    def __init__(self, physics_calculator, loss_weights=None):\n",
    "        super().__init__()\n",
    "        self.physics_calculator = physics_calculator\n",
    "        \n",
    "        # Default loss weights\n",
    "        self.loss_weights = loss_weights or {\n",
    "            'data': 1.0,\n",
    "            'physics': 1.0,\n",
    "            'boundary': 1.0\n",
    "        }\n",
    "        \n",
    "        # Loss functions\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, model, inputs, targets, physics_points=None, boundary_data=None):\n",
    "        \"\"\"Compute complete PINN loss\"\"\"\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        loss_components = {}\n",
    "        \n",
    "        # 1. Data Loss\n",
    "        predictions = model(inputs)\n",
    "        data_loss = self.mse_loss(predictions, targets)\n",
    "        total_loss += self.loss_weights['data'] * data_loss\n",
    "        loss_components['data'] = data_loss.item()\n",
    "        \n",
    "        # 2. Physics Loss\n",
    "        if physics_points is not None:\n",
    "            try:\n",
    "                physics_loss = self.physics_calculator.compute_pde_loss(\n",
    "                    model(physics_points), physics_points\n",
    "                )\n",
    "                total_loss += self.loss_weights['physics'] * physics_loss\n",
    "                loss_components['physics'] = physics_loss.item()\n",
    "            except Exception as e:\n",
    "                # Simplified physics loss for demonstration\n",
    "                physics_pred = model(physics_points)\n",
    "                # Simple regularization as physics constraint\n",
    "                physics_loss = torch.mean(torch.abs(physics_pred))\n",
    "                total_loss += self.loss_weights['physics'] * physics_loss\n",
    "                loss_components['physics'] = physics_loss.item()\n",
    "        \n",
    "        # 3. Boundary Loss\n",
    "        if boundary_data is not None:\n",
    "            boundary_points, boundary_values = boundary_data\n",
    "            boundary_pred = model(boundary_points)\n",
    "            boundary_loss = self.mse_loss(boundary_pred, boundary_values)\n",
    "            total_loss += self.loss_weights['boundary'] * boundary_loss\n",
    "            loss_components['boundary'] = boundary_loss.item()\n",
    "        \n",
    "        loss_components['total'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, loss_components\n",
    "\n",
    "# Initialize complete loss function\n",
    "loss_weights = {\n",
    "    'data': 1.0,\n",
    "    'physics': 0.1,  # Start with lower physics weight\n",
    "    'boundary': 0.5\n",
    "}\n",
    "\n",
    "pinn_loss = CompletePINNLoss(physics_calculator, loss_weights)\n",
    "\n",
    "print(\"üéØ Complete PINN Loss Function Created\")\n",
    "print(f\"   Data weight: {loss_weights['data']}\")\n",
    "print(f\"   Physics weight: {loss_weights['physics']}\")\n",
    "print(f\"   Boundary weight: {loss_weights['boundary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PINN Training Setup\n",
    "\n",
    "Let's set up the training process and demonstrate a few training steps to see how the PINN learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "def prepare_training_tensors(pinn_data, device):\n",
    "    \"\"\"Convert numpy arrays to PyTorch tensors\"\"\"\n",
    "    \n",
    "    train_data = pinn_data['train']\n",
    "    val_data = pinn_data['validation']\n",
    "    \n",
    "    # Training tensors\n",
    "    X_train = torch.FloatTensor(train_data['X']).to(device)\n",
    "    y_train = torch.FloatTensor(train_data['y']).to(device)\n",
    "    \n",
    "    # Validation tensors\n",
    "    X_val = torch.FloatTensor(val_data['X']).to(device)\n",
    "    y_val = torch.FloatTensor(val_data['y']).to(device)\n",
    "    \n",
    "    # Physics points (subset of training data)\n",
    "    n_physics = min(500, len(X_train))\n",
    "    physics_idx = torch.randperm(len(X_train))[:n_physics]\n",
    "    X_physics = X_train[physics_idx].clone()\n",
    "    X_physics.requires_grad_(True)\n",
    "    \n",
    "    # Boundary points (example: top and bottom depths)\n",
    "    depth_min, depth_max = torch.min(X_train[:, 0]), torch.max(X_train[:, 0])\n",
    "    \n",
    "    # Create boundary conditions\n",
    "    n_boundary = 50\n",
    "    boundary_points = X_train[:n_boundary].clone()\n",
    "    \n",
    "    # Set boundary values (example: fixed pressure at boundaries)\n",
    "    boundary_values = torch.zeros(n_boundary, 2, device=device)\n",
    "    boundary_values[:, 0] = 100.0  # Fixed pressure\n",
    "    boundary_values[:, 1] = 0.5    # Fixed saturation\n",
    "    \n",
    "    return {\n",
    "        'train': (X_train, y_train),\n",
    "        'validation': (X_val, y_val),\n",
    "        'physics': X_physics,\n",
    "        'boundary': (boundary_points, boundary_values)\n",
    "    }\n",
    "\n",
    "# Prepare tensors\n",
    "training_tensors = prepare_training_tensors(pinn_data, device)\n",
    "\n",
    "print(\"üì¶ Training Tensors Prepared:\")\n",
    "print(f\"  Training data: {training_tensors['train'][0].shape} ‚Üí {training_tensors['train'][1].shape}\")\n",
    "print(f\"  Validation data: {training_tensors['validation'][0].shape} ‚Üí {training_tensors['validation'][1].shape}\")\n",
    "print(f\"  Physics points: {training_tensors['physics'].shape}\")\n",
    "print(f\"  Boundary points: {training_tensors['boundary'][0].shape} ‚Üí {training_tensors['boundary'][1].shape}\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(pinn_model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "print(f\"\\nüéõÔ∏è Optimizer: Adam (lr=1e-3)\")\n",
    "print(f\"üìâ Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demonstration Training Loop\n",
    "\n",
    "Let's run a short training demonstration to see how the PINN learns to satisfy both data and physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstration_training(model, loss_fn, optimizer, training_tensors, n_epochs=50):\n",
    "    \"\"\"Demonstration training loop for PINN\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Starting Demonstration Training ({n_epochs} epochs)\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Unpack training data\n",
    "    X_train, y_train = training_tensors['train']\n",
    "    X_val, y_val = training_tensors['validation']\n",
    "    X_physics = training_tensors['physics']\n",
    "    boundary_data = training_tensors['boundary']\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'data_loss': [],\n",
    "        'physics_loss': [],\n",
    "        'boundary_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute PINN loss\n",
    "        total_loss, loss_components = loss_fn(\n",
    "            model, X_train, y_train, X_physics, boundary_data\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = torch.nn.functional.mse_loss(val_pred, y_val)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(loss_components['total'])\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['data_loss'].append(loss_components['data'])\n",
    "        history['physics_loss'].append(loss_components.get('physics', 0))\n",
    "        history['boundary_loss'].append(loss_components.get('boundary', 0))\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: \"\n",
    "                  f\"Total={loss_components['total']:.6f}, \"\n",
    "                  f\"Data={loss_components['data']:.6f}, \"\n",
    "                  f\"Physics={loss_components.get('physics', 0):.6f}, \"\n",
    "                  f\"Boundary={loss_components.get('boundary', 0):.6f}, \"\n",
    "                  f\"Val={val_loss.item():.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run demonstration training\n",
    "training_history = demonstration_training(\n",
    "    pinn_model, pinn_loss, optimizer, training_tensors, n_epochs=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Demonstration training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Progress Visualization\n",
    "\n",
    "Let's visualize how the different loss components evolve during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_progress(history):\n",
    "    \"\"\"Visualize PINN training progress\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # 1. Total loss comparison\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r--', label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Total Loss Evolution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # 2. Loss components\n",
    "    axes[0, 1].plot(epochs, history['data_loss'], 'g-', label='Data Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['physics_loss'], 'b-', label='Physics Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['boundary_loss'], 'm-', label='Boundary Loss', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss Component')\n",
    "    axes[0, 1].set_title('Loss Components')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # 3. Loss ratios\n",
    "    data_ratio = np.array(history['data_loss']) / np.array(history['train_loss'])\n",
    "    physics_ratio = np.array(history['physics_loss']) / np.array(history['train_loss'])\n",
    "    boundary_ratio = np.array(history['boundary_loss']) / np.array(history['train_loss'])\n",
    "    \n",
    "    axes[1, 0].plot(epochs, data_ratio, 'g-', label='Data Fraction', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, physics_ratio, 'b-', label='Physics Fraction', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, boundary_ratio, 'm-', label='Boundary Fraction', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss Fraction')\n",
    "    axes[1, 0].set_title('Loss Component Ratios')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Learning rate and convergence\n",
    "    # Compute loss reduction rate\n",
    "    loss_reduction = np.diff(np.log(history['train_loss']))\n",
    "    \n",
    "    axes[1, 1].plot(epochs[1:], -loss_reduction, 'purple', linewidth=2, label='Loss Reduction Rate')\n",
    "    axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='No Improvement')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss Reduction Rate')\n",
    "    axes[1, 1].set_title('Training Convergence')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('PINN Training Progress Analysis', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"\\nüìä Training Statistics:\")\n",
    "    print(f\"  Initial total loss: {history['train_loss'][0]:.6f}\")\n",
    "    print(f\"  Final total loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"  Loss reduction: {(1 - history['train_loss'][-1]/history['train_loss'][0])*100:.1f}%\")\n",
    "    print(f\"  Final validation loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final data loss: {history['data_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final physics loss: {history['physics_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final boundary loss: {history['boundary_loss'][-1]:.6f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "visualize_training_progress(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Predictions and Physics Validation\n",
    "\n",
    "Let's examine the trained model's predictions and validate that it satisfies physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_predictions(model, training_tensors, device):\n",
    "    \"\"\"Analyze model predictions and physics compliance\"\"\"\n",
    "    \n",
    "    print(\"üîç Analyzing Model Predictions\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get test data\n",
    "    X_test, y_test = training_tensors['validation']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Model predictions\n",
    "        y_pred = model(X_test)\n",
    "        \n",
    "        # Convert to numpy for analysis\n",
    "        X_np = X_test.cpu().numpy()\n",
    "        y_true_np = y_test.cpu().numpy()\n",
    "        y_pred_np = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse_pressure = np.mean((y_true_np[:, 0] - y_pred_np[:, 0])**2)\n",
    "    mse_saturation = np.mean((y_true_np[:, 1] - y_pred_np[:, 1])**2)\n",
    "    \n",
    "    mae_pressure = np.mean(np.abs(y_true_np[:, 0] - y_pred_np[:, 0]))\n",
    "    mae_saturation = np.mean(np.abs(y_true_np[:, 1] - y_pred_np[:, 1]))\n",
    "    \n",
    "    r2_pressure = 1 - mse_pressure / np.var(y_true_np[:, 0])\n",
    "    r2_saturation = 1 - mse_saturation / np.var(y_true_np[:, 1])\n",
    "    \n",
    "    print(f\"üìà Prediction Metrics:\")\n",
    "    print(f\"  Pressure - MSE: {mse_pressure:.6f}, MAE: {mae_pressure:.6f}, R¬≤: {r2_pressure:.4f}\")\n",
    "    print(f\"  Saturation - MSE: {mse_saturation:.6f}, MAE: {mae_saturation:.6f}, R¬≤: {r2_saturation:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Pressure predictions\n",
    "    axes[0, 0].scatter(y_true_np[:, 0], y_pred_np[:, 0], alpha=0.6, s=20)\n",
    "    min_p, max_p = np.min(y_true_np[:, 0]), np.max(y_true_np[:, 0])\n",
    "    axes[0, 0].plot([min_p, max_p], [min_p, max_p], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('True Pressure')\n",
    "    axes[0, 0].set_ylabel('Predicted Pressure')\n",
    "    axes[0, 0].set_title(f'Pressure Predictions (R¬≤ = {r2_pressure:.3f})')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Saturation predictions\n",
    "    axes[0, 1].scatter(y_true_np[:, 1], y_pred_np[:, 1], alpha=0.6, s=20, color='orange')\n",
    "    min_s, max_s = np.min(y_true_np[:, 1]), np.max(y_true_np[:, 1])\n",
    "    axes[0, 1].plot([min_s, max_s], [min_s, max_s], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0, 1].set_xlabel('True Saturation')\n",
    "    axes[0, 1].set_ylabel('Predicted Saturation')\n",
    "    axes[0, 1].set_title(f'Saturation Predictions (R¬≤ = {r2_saturation:.3f})')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual analysis\n",
    "    pressure_residuals = y_true_np[:, 0] - y_pred_np[:, 0]\n",
    "    axes[0, 2].hist(pressure_residuals, bins=30, alpha=0.7, color='blue', density=True)\n",
    "    axes[0, 2].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Pressure Residuals')\n",
    "    axes[0, 2].set_ylabel('Density')\n",
    "    axes[0, 2].set_title('Pressure Residual Distribution')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Depth vs predictions\n",
    "    depth = X_np[:, 0]\n",
    "    axes[1, 0].scatter(depth, y_pred_np[:, 0], alpha=0.6, s=20, label='Predicted', color='blue')\n",
    "    axes[1, 0].scatter(depth, y_true_np[:, 0], alpha=0.6, s=20, label='True', color='red')\n",
    "    axes[1, 0].set_xlabel('Depth')\n",
    "    axes[1, 0].set_ylabel('Pressure')\n",
    "    axes[1, 0].set_title('Pressure vs Depth')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Porosity vs predictions\n",
    "    porosity = X_np[:, 2]\n",
    "    axes[1, 1].scatter(porosity, y_pred_np[:, 1], alpha=0.6, s=20, label='Predicted', color='orange')\n",
    "    axes[1, 1].scatter(porosity, y_true_np[:, 1], alpha=0.6, s=20, label='True', color='red')\n",
    "    axes[1, 1].set_xlabel('Porosity')\n",
    "    axes[1, 1].set_ylabel('Saturation')\n",
    "    axes[1, 1].set_title('Saturation vs Porosity')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Error vs input features\n",
    "    pressure_error = np.abs(pressure_residuals)\n",
    "    axes[1, 2].scatter(depth, pressure_error, alpha=0.6, s=20)\n",
    "    axes[1, 2].set_xlabel('Depth')\n",
    "    axes[1, 2].set_ylabel('Absolute Pressure Error')\n",
    "    axes[1, 2].set_title('Prediction Error vs Depth')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('PINN Model Prediction Analysis', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'predictions': y_pred_np,\n",
    "        'true_values': y_true_np,\n",
    "        'metrics': {\n",
    "            'pressure': {'mse': mse_pressure, 'mae': mae_pressure, 'r2': r2_pressure},\n",
    "            'saturation': {'mse': mse_saturation, 'mae': mae_saturation, 'r2': r2_saturation}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Analyze model predictions\n",
    "prediction_results = analyze_model_predictions(pinn_model, training_tensors, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Insights\n",
    "\n",
    "In this tutorial, we've successfully implemented a complete Physics-Informed Neural Network for reservoir modeling.\n",
    "\n",
    "### Key Accomplishments\n",
    "1. **PINN Architecture**: Built a neural network suitable for physics-informed learning\n",
    "2. **Physics Integration**: Implemented automatic differentiation for PDE residuals\n",
    "3. **Complete Loss Function**: Combined data fitting with physics constraints\n",
    "4. **Training Demonstration**: Showed how PINNs learn from both data and physics\n",
    "5. **Validation**: Analyzed model predictions and physics compliance\n",
    "\n",
    "### PINN Implementation Insights\n",
    "\n",
    "#### Architecture Design\n",
    "- **Smooth Activations**: Tanh functions enable smooth derivatives for physics\n",
    "- **Appropriate Depth**: 3-4 hidden layers balance expressiveness and trainability\n",
    "- **Neuron Count**: 50-100 neurons per layer sufficient for many problems\n",
    "\n",
    "#### Physics Integration\n",
    "- **Automatic Differentiation**: PyTorch's autograd enables seamless PDE computation\n",
    "- **Loss Weighting**: Balance between data fitting and physics constraints is crucial\n",
    "- **Gradient Management**: Proper gradient clipping prevents training instability\n",
    "\n",
    "#### Training Strategy\n",
    "- **Multi-objective Optimization**: Simultaneously minimize data and physics losses\n",
    "- **Adaptive Weights**: May need to adjust loss weights during training\n",
    "- **Convergence Monitoring**: Track all loss components separately\n",
    "\n",
    "### Advantages Demonstrated\n",
    "1. **Physics Consistency**: Model predictions respect governing equations\n",
    "2. **Data Efficiency**: Can work with limited training data\n",
    "3. **Interpretability**: Physics constraints provide interpretable structure\n",
    "4. **Generalization**: Physics helps model generalize beyond training data\n",
    "\n",
    "### Common Challenges\n",
    "1. **Loss Balancing**: Finding optimal weights for different loss terms\n",
    "2. **Training Stability**: Physics losses can cause gradient issues\n",
    "3. **Computational Cost**: Automatic differentiation increases computation\n",
    "4. **Hyperparameter Sensitivity**: More hyperparameters to tune than standard NNs\n",
    "\n",
    "### Next Steps\n",
    "In the following tutorials, we will:\n",
    "1. **Tutorial 4**: Implement advanced training strategies and optimization\n",
    "2. **Tutorial 5**: Comprehensive validation and benchmarking\n",
    "3. **Tutorial 6**: Advanced topics and extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model for next tutorials\n",
    "output_dir = Path('../output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model state\n",
    "torch.save({\n",
    "    'model_state_dict': pinn_model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'training_history': training_history,\n",
    "    'prediction_results': prediction_results,\n",
    "    'loss_weights': loss_weights\n",
    "}, output_dir / 'pinn_model_demo.pth')\n",
    "\n",
    "print(\"üíæ Model and results saved successfully!\")\n",
    "print(f\"üìÅ Location: {output_dir / 'pinn_model_demo.pth'}\")\n",
    "\n",
    "print(\"\\nüéâ Tutorial 3 Complete!\")\n",
    "print(\"\\nüìã Summary of Implementation:\")\n",
    "print(f\"  ‚úÖ PINN architecture with {sum(p.numel() for p in pinn_model.parameters()):,} parameters\")\n",
    "print(f\"  ‚úÖ Physics-informed loss function with automatic differentiation\")\n",
    "print(f\"  ‚úÖ Training demonstration with {len(training_history['train_loss'])} epochs\")\n",
    "print(f\"  ‚úÖ Model validation with R¬≤ = {prediction_results['metrics']['pressure']['r2']:.3f} (pressure)\")\n",
    "print(f\"  ‚úÖ Physics constraint integration and monitoring\")\n",
    "\n",
    "print(\"\\n‚û°Ô∏è  Next: Tutorial 4 - Advanced Training Strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.5"\n",
  }\n",
 },\n",
 "nbformat": 4,\n",
 "nbformat_minor": 4\n}